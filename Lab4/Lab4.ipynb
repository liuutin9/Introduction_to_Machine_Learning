{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IagZMs0_qjdL"
   },
   "source": [
    "# **Lab 4 : Neural Network**\n",
    "\n",
    "In *lab 4*, you need to finish:\n",
    "\n",
    "1. Basic Part (65%):\n",
    "  Implement a deep neural network from scratch\n",
    "\n",
    "  > * Section 1: Neural network implementation\n",
    "    >> * Part 1: Linear layer\n",
    "    >> * Part 2: Activation function layer\n",
    "    >> * Part 3: Build model\n",
    "\n",
    "  > * Section 2: Loss function\n",
    "    >> * Part 1: Binary cross-entropy loss (BCE)\n",
    "    >> * Part 2: Categorical cross-entropy loss (CCE)\n",
    "    >> * Part 3: Mean square error (MSE)\n",
    "  > * Section 3: Training and prediction\n",
    "    >> * Part 1: Training function & batch function\n",
    "    >> * Part 2: Regression\n",
    "    >> * Part 3: Binary classification\n",
    "\n",
    "\n",
    "2. Advanced Part (30%): Multi class classification\n",
    "3. Report (5%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGFR00CQvoaH"
   },
   "source": [
    "## **Important  notice**\n",
    "\n",
    "* Please **do not** change the code outside this code bracket in the basic part.\n",
    "  ```\n",
    "  ### START CODE HERE ###\n",
    "  ...\n",
    "  ### END CODE HERE ###\n",
    "  ```\n",
    "\n",
    "* Please **do not** import any other packages in both basic and advanced part\n",
    "\n",
    "* Please **do not** change the random seed **np.random.seed(1)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BgcgLVV79Bm"
   },
   "source": [
    "## Import Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fmTH9UkeqdYf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import cupy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.metrics import f1_score\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "outputs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tO31dEFx-C1y"
   },
   "source": [
    "### Common Notation\n",
    "  * $C$: number of classes\n",
    "  * $n$: number of samples\n",
    "  * $f^{[l]}$: the dimension of outputs in layer $l$, but $f^{[0]}$ is the input dimension\n",
    "  * $Z^{[l]} = A^{[l-1]}W^{[l]} + b^{[l]}$\n",
    "      * $Z^{[l]}$: the output of layer $l$ in the shape $(n, f^{[l]})$\n",
    "      * $A^{[l]}$: the activation of $Z^{[l]}$ in the shape $(n, f^{[l]})$, but $A^{[0]}$ is input $X$\n",
    "      * $W^{[l]}$: the weight in layer $l$ in the shape $(f^{[l-1]}, f^{[l]})$\n",
    "      * $b^{[l]}$: the bias in layer $l$ in the shape $(1, f^{[l]})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wE5z0w8FQLK"
   },
   "source": [
    "# **Basic Part (65%)**\n",
    "In the Basic Part, you will implement a neural network framework capable of handling both regression, binary classification and multi-class classification tasks.\n",
    "\n",
    "**Note:**\n",
    "After implementing each class/function, test it with the provided input variables to verify its correctness. Save the results in the **outputs** dictionary. (The code for testing and saving results is already provided.)\n",
    "## Section 1: Neural network implementation\n",
    "* Part 1: Linear layer\n",
    "> * Step 1: Linear Initialize parameters\n",
    "> * Step 2: Linear forward\n",
    "> * Step 3: Linear backward\n",
    "> * Step 4: Linear update parameters\n",
    "* Part 2: Activation function layer\n",
    "> * Step 1: Activation forward\n",
    "> * Step 2: Activation backward\n",
    "* Part 3: Build model\n",
    "> * Step 1: Model Initialize parameters\n",
    "> * Step 2: Model forward\n",
    "> * Step 3: Model backward\n",
    "> * Step 4: Model update parameters\n",
    "\n",
    "## Section 2: Loss function\n",
    "* Part 1: Binary cross-entropy loss (BCE)\n",
    "* Part 2: Categorical cross-entropy loss (CCE)\n",
    "* Part 3: Mean square error (MSE)\n",
    "\n",
    "## Section 3: Training and prediction\n",
    "* Part 1: Training function & batch function\n",
    "* Part 2: Regression\n",
    "* Part 3: Binary classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w35ZkTwMc00G"
   },
   "source": [
    "## **Section 1: Neural network implementation(30%)**\n",
    "To implement a neural network, you need to complete 3 classes: **Dense**, **Activation**, and **Model**.\n",
    "The process of training a deep neural network is composed of 3 steps: *forward propagation*, *backward propagation*, and *update*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_krGKUNg_Ix"
   },
   "source": [
    "## Part 1: Linear layer (10%)\n",
    "Dense layer (fully-connected layer) performs linear transformation:\n",
    "\n",
    "$Z = AW + b$, where W is weight matrix and b is bias vector.\n",
    "\n",
    "> ### Step 1: Initialize parameters (0%)\n",
    " * You don't need to write this part.\n",
    " * W is randomly initialized using uniform distribution within $[\\text\\{-limit\\}, \\text\\{limit\\}]$, where $\\text\\{limit\\} = \\sqrt{\\frac{6}{\\text\\{fanin\\} + \\text\\{fanout\\}}}$ (fanin: number of input features, fanout: number of output features)\n",
    " * b is initialized to 0\n",
    "\n",
    "> ### Step 2: Linear forward (4%)\n",
    "* Compute Z using matrix multiplication and addition\n",
    "\n",
    "> ### Step 3: Linear backward (4%)\n",
    "* Use backpropagation to compute gradients of loss function with respect to parameters\n",
    "* For layer l: $Z^{[l]} = A^{[l-1]} W^{[l]} + b^{[l]}$ (followed by activation)\n",
    "* Given $dZ^{[l]}$ (gradient of loss with respect to Z), we need to compute three gradients:\n",
    "  * $dW^{[l]}$: gradient of loss with respect to weights\n",
    "  * $db^{[l]}$: gradient of loss with respect to bias\n",
    "  * $dA^{[l-1]}$: gradient of loss with respect to previous layer output\n",
    "\n",
    "> Formulas:\n",
    "$$ dW^{[l]} = \\frac{1}{n} A^{[l-1] T} dZ^{[l]} $$\n",
    "$$ db^{[l]} = \\frac{1}{n} \\sum_{i = 1}^{n} dZ_i^{[l]} $$\n",
    "$$ dA^{[l-1]} = dZ^{[l]} W^{[l] T} $$\n",
    "\n",
    "> ### Step 4: Linear update parameters (2%)\n",
    "* Update parameters using gradient descent:\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} $$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "x0KHo8w9yqbY"
   },
   "outputs": [],
   "source": [
    "class Dense():\n",
    "    def __init__(self, n_x, n_y, seed=1):\n",
    "        self.n_x = n_x\n",
    "        self.n_y = n_y\n",
    "        self.seed = seed\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        Argument:\n",
    "        self.n_x -- size of the input layer\n",
    "        self.n_y -- size of the output layer\n",
    "        self.parameters -- python dictionary containing your parameters:\n",
    "                           W -- weight matrix of shape (n_x, n_y)\n",
    "                           b -- bias vector of shape (1, n_y)\n",
    "        \"\"\"\n",
    "        sd = np.sqrt(6.0 / (self.n_x + self.n_y))\n",
    "        np.random.seed(self.seed)\n",
    "        W = np.random.uniform(-sd, sd, (self.n_y, self.n_x)).T      # the transpose here is just for the code to be compatible with the old codes\n",
    "        b = np.zeros((1, self.n_y))\n",
    "\n",
    "        assert(W.shape == (self.n_x, self.n_y))\n",
    "        assert(b.shape == (1, self.n_y))\n",
    "\n",
    "        self.parameters = {\"W\": W, \"b\": b}\n",
    "\n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "        Arguments:\n",
    "        A -- activations from previous layer (or input data) with the shape (n, f^[l-1])\n",
    "        self.cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "\n",
    "        Returns:\n",
    "        Z -- the input of the activation function, also called pre-activation parameter with the shape (n, f^[l])\n",
    "        \"\"\"\n",
    "\n",
    "        # GRADED FUNCTION: linear_forward\n",
    "        ### START CODE HERE ###\n",
    "        Z = A @ self.parameters[\"W\"] + self.parameters[\"b\"]\n",
    "        self.cache = (A, self.parameters[\"W\"], self.parameters[\"b\"])\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        assert(Z.shape == (A.shape[0], self.parameters[\"W\"].shape[1]))\n",
    "\n",
    "        return Z\n",
    "\n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "        Arguments:\n",
    "        dZ -- Gradient of the loss with respect to the linear output (of current layer l), same shape as Z\n",
    "        self.cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "        self.dW -- Gradient of the loss with respect to W (current layer l), same shape as W\n",
    "        self.db -- Gradient of the loss with respect to b (current layer l), same shape as b\n",
    "\n",
    "        Returns:\n",
    "        dA_prev -- Gradient of the loss with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "\n",
    "        \"\"\"\n",
    "        A_prev, W, b = self.cache\n",
    "        m = A_prev.shape[0]\n",
    "\n",
    "        # GRADED FUNCTION: linear_backward\n",
    "        ### START CODE HERE ###\n",
    "        self.dW = A_prev.T @ dZ / m\n",
    "        self.db = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "        dA_prev = dZ @ W.T\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        assert (dA_prev.shape == A_prev.shape)\n",
    "        assert (self.dW.shape == self.parameters[\"W\"].shape)\n",
    "        assert (self.db.shape == self.parameters[\"b\"].shape)\n",
    "\n",
    "        return dA_prev\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Update parameters using gradient descent\n",
    "\n",
    "        Arguments:\n",
    "        learning rate -- step size\n",
    "        \"\"\"\n",
    "\n",
    "        # GRADED FUNCTION: linear_update_parameters\n",
    "        ### START CODE HERE ###\n",
    "        self.parameters[\"W\"] = self.parameters[\"W\"] - learning_rate * self.dW\n",
    "        self.parameters[\"b\"] = self.parameters[\"b\"] - learning_rate * self.db\n",
    "        ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbnVsi6VJMXD"
   },
   "source": [
    "### Test your **Dense class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7HNAWwmg8R7T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = [[-0.20325375]\n",
      " [ 0.53968259]\n",
      " [-1.22446471]]\n",
      "b = [[0.]]\n",
      "Z = [[1.9]\n",
      " [2.2]\n",
      " [2.5]]\n",
      "dA_prev = [[3.5]\n",
      " [6. ]]\n",
      "dW = [[1.625 0.625]]\n",
      "db = [[2.   0.75]]\n",
      "W = [[0.5 2.5]]\n",
      "b = [[-1.  2.]]\n"
     ]
    }
   ],
   "source": [
    "# Initial parameters\n",
    "dense = Dense(3, 1)\n",
    "print(\"W = \" + str(dense.parameters[\"W\"]))\n",
    "print(\"b = \" + str(dense.parameters[\"b\"]))\n",
    "\n",
    "# Linear forward\n",
    "A, W, b = np.array([[0., 1., 2.], [0.5, 1.5, 2.5], [1., 2., 3.]]), np.array([[0.1], [0.2], [0.3]]), np.array([[1.1]])\n",
    "dense = Dense(3, 1)\n",
    "dense.parameters = {\"W\": W, \"b\": b}\n",
    "Z = dense.forward(A)\n",
    "print(\"Z = \" + str(Z))\n",
    "\n",
    "A, W, b = np.array([[-0.80,-0.45,-1.11],[-1.65,-2.36,1.14],[-1.02,0.64,-0.86]]), np.array([[0.3], [0.3], [0.1]]), np.array([[-6.2]])\n",
    "dense = Dense(3, 1)\n",
    "dense.parameters = {\"W\": W, \"b\": b}\n",
    "Z = dense.forward(A)\n",
    "outputs[\"dense_forward\"] = (Z, dense.cache)\n",
    "\n",
    "# Linear backward\n",
    "dZ, linear_cache = np.array([[1.5, 0.5], [2.5, 1.]]), (np.array([[0.5], [1]]), np.array([[2., 1.0]]), np.array([[0.5, 1.]]))\n",
    "dense = Dense(1, 2)\n",
    "dense.cache = linear_cache\n",
    "dA_prev = dense.backward(dZ)\n",
    "print (\"dA_prev = \" + str(dA_prev))\n",
    "print (\"dW = \" + str(dense.dW))\n",
    "print (\"db = \" + str(dense.db))\n",
    "\n",
    "dZ, linear_cache = np.array([[0.52,0.34],[0.76,0.89]]), (np.array([[0.42], [0.68]]), np.array([[0.35, 0.89]]), np.array([[0.12, 0.76]]))\n",
    "dense = Dense(1, 2)\n",
    "dense.cache = linear_cache\n",
    "dA_prev = dense.backward(dZ)\n",
    "outputs[\"dense_backward\"] = (dA_prev, dense.dW, dense.db)\n",
    "\n",
    "# Linear update parameters\n",
    "np.random.seed(1)\n",
    "dense = Dense(1, 2)\n",
    "dense.parameters = {\"W\": np.array([[1.0, 2.0]]), \"b\": np.array([[0.5, 0.5]])}\n",
    "dense.dW = np.array([[0.5, -0.5]])\n",
    "dense.db = np.array([[1.5, -1.5]])\n",
    "dense.update(1.0)\n",
    "print(\"W = \" + str(dense.parameters[\"W\"]))\n",
    "print(\"b = \" + str(dense.parameters[\"b\"]))\n",
    "\n",
    "np.random.seed(1)\n",
    "dense = Dense(3, 4)\n",
    "parameters, grads = {\"W1\": np.random.rand(3, 4), \"b1\": np.random.rand(1,4)}, {\"dW1\": np.random.rand(3, 4), \"db1\": np.random.rand(1,4)}\n",
    "dense.parameters = {\"W\": parameters[\"W1\"], \"b\": parameters[\"b1\"]}\n",
    "dense.dW = grads[\"dW1\"]\n",
    "dense.db = grads[\"db1\"]\n",
    "dense.update(0.1)\n",
    "outputs[\"dense_update_parameters\"] = {\"W\": dense.parameters[\"W\"], \"b\": dense.parameters[\"b\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtPtH0j3BFN7"
   },
   "source": [
    "Expected output:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>W: </td>\n",
    "    <td>[[-0.20325375]  [0.53968259 [-1.22446471]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>b: </td>\n",
    "    <td>[[0.]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Z: </td>\n",
    "    <td>[[1.9] [2.2] [2.5]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dA_prev: </td>\n",
    "    <td>[[3.5] [6.0]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dW: </td>\n",
    "    <td>[[1.625 0.625]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>db: </td>\n",
    "    <td>[[2.0 0.75]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>W: </td>\n",
    "    <td>[[0.5 2.5]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>b: </td>\n",
    "    <td>[[-1.  2.]]</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2r5m2W3aXh_A"
   },
   "source": [
    "## Part 2: Activation function layer (10%)\n",
    "\n",
    "Implement forward and backward propagation for activation function layers, including Sigmoid, Softmax, and ReLU.\n",
    "\n",
    "> ### Step 1: Forward Propagation (5%)\n",
    " Implement the following activation functions:\n",
    ">> #### a) Sigmoid\n",
    "- Use the numerically stable version to prevent exponential overflow:\n",
    "  $$\\sigma(Z) = \\begin{cases}\n",
    "    \\frac{1}{1+e^{-Z}},& \\text{if } Z \\geq 0\\\\\n",
    "    \\frac{e^{Z}}{1+e^{Z}}, & \\text{otherwise}\n",
    "  \\end{cases}$$\n",
    "\n",
    ">> #### b) ReLU\n",
    "- Simple implementation:\n",
    "  $$RELU(Z) = \\max(Z, 0)$$\n",
    "\n",
    ">> #### c) Softmax\n",
    "- Implement using the numerically stable version:\n",
    "  $$\\sigma(\\vec{Z})_i = \\frac{e^{Z_i-b}}{\\sum_{j=1}^{C} e^{Z_j-b}}$$\n",
    "  where $b = \\max_{j=1}^{C} Z_j$\n",
    "\n",
    ">> #### d) Linear\n",
    "- You don't need to implement this part\n",
    "\n",
    "> ### Requirements\n",
    "- Each function should return:\n",
    "  1. Activation value \"a\"\n",
    "  2. Cache containing \"z\" for backward propagation\n",
    "\n",
    "> ### Step 2: Backward Propagation (5%)\n",
    "Implement backward functions for:\n",
    "- Sigmoid\n",
    "- ReLU\n",
    "- Softmax\n",
    "- linear\n",
    "\n",
    "> ### General Form\n",
    "$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]})$$\n",
    "where $g(.)$ is the activation function\n",
    "\n",
    "> ### Specific Implementations\n",
    "\n",
    ">> #### a) Sigmoid Backward\n",
    "$$\\sigma'(Z^{[l]}) = \\sigma(Z^{[l]}) (1 - \\sigma(Z^{[l]}))$$\n",
    "Use numerically stable sigmoid\n",
    "\n",
    ">> #### b) ReLU Backward\n",
    "$$g'(Z^{[l]}) = \\begin{cases}\n",
    "    1,& \\text{if } Z^{[l]} > 0\\\\\n",
    "    0,              & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    ">> #### c) Softmax Backward\n",
    "For the special case of Softmax combined with Categorical Cross-Entropy loss:\n",
    "$$dZ^{[l]} = s - y$$\n",
    "where $s$ is softmax output, $y$ is true label (one-hot vector)\n",
    "\n",
    "Note: This is a simplified form specific to Softmax + CCE loss combination.\n",
    "\n",
    ">> #### d) linear Backward\n",
    "You don't need to implement this part\n",
    "\n",
    "> ### Note\n",
    "For softmax, use the normalized exponential function to prevent overflow, but use the simplified gradient equation for backwards propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Nnuv8MmebMgg"
   },
   "outputs": [],
   "source": [
    "class Activation():\n",
    "    def __init__(self, activation_function, loss_function):\n",
    "        self.activation_function = activation_function\n",
    "        self.loss_function = loss_function\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, Z):\n",
    "        if self.activation_function == \"sigmoid\":\n",
    "            \"\"\"\n",
    "            Implements the sigmoid activation in numpy\n",
    "\n",
    "            Arguments:\n",
    "            Z -- numpy array of any shape\n",
    "            self.cache -- stores Z as well, useful during backpropagation\n",
    "\n",
    "            Returns:\n",
    "            A -- output of sigmoid(z), same shape as Z\n",
    "            \"\"\"\n",
    "\n",
    "            # GRADED FUNCTION: sigmoid_forward\n",
    "            ### START CODE HERE ###\n",
    "            A = np.where(Z >= 0, 1 / (1 + np.exp(-Z)), np.exp(Z) / (1 + np.exp(Z)))\n",
    "            self.cache = Z\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            return A\n",
    "        elif self.activation_function == \"relu\":\n",
    "            \"\"\"\n",
    "            Implement the RELU function in numpy\n",
    "            Arguments:\n",
    "            Z -- numpy array of any shape\n",
    "            self.cache -- stores Z as well, useful during backpropagation\n",
    "            Returns:\n",
    "            A -- output of relu(z), same shape as Z\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            # GRADED FUNCTION: relu_forward\n",
    "            ### START CODE HERE ###\n",
    "            A = np.where(Z >= 0, Z, 0)\n",
    "            self.cache = Z\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            assert(A.shape == Z.shape)\n",
    "\n",
    "            return A\n",
    "        elif self.activation_function == \"softmax\":\n",
    "            \"\"\"\n",
    "            Implements the softmax activation in numpy\n",
    "\n",
    "            Arguments:\n",
    "            Z -- np.array with shape (n, C)\n",
    "            self.cache -- stores Z as well, useful during backpropagation\n",
    "\n",
    "            Returns:\n",
    "            A -- output of softmax(z), same shape as Z\n",
    "            \"\"\"\n",
    "\n",
    "            # GRADED FUNCTION: softmax_forward\n",
    "            ### START CODE HERE ###\n",
    "            b = np.max(Z, axis=1, keepdims=True)\n",
    "            A = np.exp(Z - b) / np.sum(np.exp(Z - b), axis=1, keepdims=True)\n",
    "            self.cache = Z\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            return A\n",
    "        elif self.activation_function == \"linear\":\n",
    "            \"\"\"\n",
    "            Linear activation (returns Z directly).\n",
    "            \"\"\"\n",
    "            self.cache = Z.copy()\n",
    "            return Z\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {self.activation_function}\")\n",
    "\n",
    "\n",
    "    def backward(self, dA=None, Y=None):\n",
    "        if self.activation_function == \"sigmoid\":\n",
    "            \"\"\"\n",
    "            Implement the backward propagation for a single SIGMOID unit.\n",
    "            Arguments:\n",
    "            dA -- post-activation gradient, of any shape\n",
    "            self.cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "            Returns:\n",
    "            dZ -- Gradient of the loss with respect to Z\n",
    "            \"\"\"\n",
    "\n",
    "            # GRADED FUNCTION: sigmoid_backward\n",
    "            ### START CODE HERE ###\n",
    "            Z = np.where(self.cache >= 0, 1 / (1 + np.exp(-self.cache)), np.exp(self.cache) / (1 + np.exp(self.cache)))\n",
    "            dZ = dA * (Z * (1 - Z))\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            assert (dZ.shape == Z.shape)\n",
    "\n",
    "            return dZ\n",
    "\n",
    "        elif self.activation_function == \"relu\":\n",
    "            \"\"\"\n",
    "            Implement the backward propagation for a single RELU unit.\n",
    "            Arguments:\n",
    "            dA -- post-activation gradient, of any shape\n",
    "            self.cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "            Returns:\n",
    "            dZ -- Gradient of the loss with respect to Z\n",
    "            \"\"\"\n",
    "\n",
    "            # GRADED FUNCTION: relu_backward\n",
    "            ### START CODE HERE ###\n",
    "            Z = self.cache\n",
    "            dZ = dA * np.where(Z > 0, 1, 0)\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            assert (dZ.shape == Z.shape)\n",
    "\n",
    "            return dZ\n",
    "\n",
    "        elif self.activation_function == \"softmax\":\n",
    "            \"\"\"\n",
    "            Implement the backward propagation for a [SOFTMAX->CCE LOSS] unit.\n",
    "            Arguments:\n",
    "            Y -- true \"label\" vector (one hot vector, for example: [1,0,0] represents rock, [0,1,0] represents paper, [0,0,1] represents scissors\n",
    "                                      in a Rock-Paper-Scissors, shape: (n, C)\n",
    "            self.cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "            Returns:\n",
    "            dZ -- Gradient of the cost with respect to Z\n",
    "            \"\"\"\n",
    "\n",
    "            # GRADED FUNCTION: softmax_backward\n",
    "            ### START CODE HERE ###\n",
    "            Z = self.cache\n",
    "            s = self.forward(Z)\n",
    "            dZ = s - Y\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            assert (dZ.shape == self.cache.shape)\n",
    "\n",
    "            return dZ\n",
    "\n",
    "        elif self.activation_function == \"linear\":\n",
    "            \"\"\"\n",
    "            Backward propagation for linear activation.\n",
    "            \"\"\"\n",
    "            return dA\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {self.activation_function}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDYVMMS2ecCx"
   },
   "source": [
    "### Test your **Activation class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gBuRAoeUC5jV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid: A = [[0.00669285]\n",
      " [0.26894142]\n",
      " [0.5       ]\n",
      " [0.73105858]\n",
      " [0.99330715]]\n",
      "ReLU: A = [[0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [5]]\n",
      "Softmax: A = \n",
      "[[0.0320586  0.08714432 0.23688282 0.64391426]\n",
      " [0.1748777  0.47536689 0.1748777  0.1748777 ]\n",
      " [0.0320586  0.08714432 0.23688282 0.64391426]]\n",
      "Linear: A = \n",
      "[[ 1  2  3  4]\n",
      " [ 0  1  0  0]\n",
      " [-2 -1  0  1]]\n",
      "Sigmoid: dZ = [[-0.5       ]\n",
      " [-0.26935835]\n",
      " [-0.11969269]\n",
      " [-0.5       ]\n",
      " [-0.73139639]]\n",
      "ReLU: dZ = [[-0.    1.7 ]\n",
      " [-0.    0.  ]\n",
      " [-1.14  3.72]]\n",
      "Softmax: dZ = [[-0.96488097  0.70538451  0.25949646]\n",
      " [ 0.09003057 -0.75527153  0.66524096]\n",
      " [ 0.01766842  0.01766842 -0.03533684]]\n",
      "Linear: dZ = \n",
      "[[ 1.2 -0.5  0.8 -0.3]\n",
      " [ 0.4  0.6 -0.9  0.2]\n",
      " [-0.1  0.5 -0.7  0.9]]\n"
     ]
    }
   ],
   "source": [
    "# Activation forward\n",
    "Z = np.array([[-5], [-1], [0], [1], [5]])\n",
    "\n",
    "sigmoid = Activation(\"sigmoid\", 'cross_entropy')\n",
    "A = sigmoid.forward(Z)\n",
    "print(\"Sigmoid: A = \" + str(A))\n",
    "A = sigmoid.forward(np.array([[0.23], [-0.67], [0.45], [0.89], [-0.10]]))\n",
    "outputs[\"sigmoid\"] = (A, sigmoid.cache)\n",
    "\n",
    "relu = Activation(\"relu\", 'cross_entropy')\n",
    "A = relu.forward(Z)\n",
    "print(\"ReLU: A = \" + str(A))\n",
    "A = relu.forward(np.array([[-0.34], [-0.76], [0.21], [-0.98], [0.54]]))\n",
    "outputs[\"relu\"] = (A, relu.cache)\n",
    "\n",
    "Z = np.array([[1, 2, 3, 4],[0, 1, 0, 0],[-2, -1, 0, 1]])\n",
    "softmax = Activation(\"softmax\", 'cross_entropy')\n",
    "A = softmax.forward(Z)\n",
    "print(\"Softmax: A = \\n\" + str(A))\n",
    "A = softmax.forward(np.array([[0.12, -0.56, 0.78, -0.34], [0.45, 0.67, -0.89, 0.23], [-0.14, 0.50, -0.76, 0.98]]))\n",
    "outputs[\"softmax\"] = (A, softmax.cache)\n",
    "\n",
    "linear = Activation(\"linear\", 'mse')\n",
    "A = linear.forward(Z)\n",
    "print(\"Linear: A = \\n\" + str(A))\n",
    "A = linear.forward(np.array([[0.12, -0.56, 0.78, -0.34], [0.45, 0.67, -0.89, 0.23], [-0.14, 0.50, -0.76, 0.98]]))\n",
    "outputs[\"linear\"] = (A, Z)  # For linear activation, cache is just Z\n",
    "\n",
    "# Activation backward\n",
    "dA, cache = np.array([[-2], [-1.37], [-1.14], [-2], [-3.72]]), np.array([[0], [1], [2], [0], [1]])\n",
    "sigmoid = Activation(\"sigmoid\", 'cross_entropy')\n",
    "sigmoid.cache = cache\n",
    "dZ = sigmoid.backward(dA=dA)\n",
    "print(\"Sigmoid: dZ = \"+ str(dZ))\n",
    "dA, cache = np.array([[9.73], [-7.56], [8.34], [-4.12], [6.89]]), np.array([[-5.45], [3.68], [-2.32], [4.51], [-9.27]])\n",
    "sigmoid.cache = cache\n",
    "outputs[\"sigmoid_backward\"] = sigmoid.backward(dA=dA)\n",
    "\n",
    "relu = Activation(\"relu\", 'cross_entropy')\n",
    "dA, cache = np.array([[-2., 1.7 ], [-1.37, 2.], [-1.14, 3.72]]), np.array([[-2, 1], [-1, 0], [2, 1]])\n",
    "relu.cache = cache\n",
    "dZ = relu.backward(dA=dA)\n",
    "print(\"ReLU: dZ = \"+ str(dZ))\n",
    "dA, cache = np.array([[7.24, -3.58], [8.93, 6.45], [-2.11, 9.87]]), np.array([[-4.76, 5.34], [1.98, -7.22], [3.67, -8.56]])\n",
    "relu.cache = cache\n",
    "outputs[\"relu_backward\"] = relu.backward(dA=dA)\n",
    "\n",
    "Y, cache = np.array([[1, 0, 0],[0, 1, 0],[0, 0, 1]]), np.array([[-2, 1, 0],[-1, 0, 1],[-2, -2, 2]])\n",
    "softmax = Activation(\"softmax\", 'cross_entropy')\n",
    "softmax.cache = cache\n",
    "dZ = softmax.backward(Y=Y)\n",
    "print(\"Softmax: dZ = \" + str(dZ))\n",
    "Y, cache = np.array([[0, 1, 0], [0, 1, 0], [1, 0, 0]]), np.array([[-9.45, 7.32, 3.58], [5.61, -8.27, 6.49], [1.23, -4.56, 7.84]])\n",
    "softmax.cache = cache\n",
    "outputs[\"softmax_backward\"] = softmax.backward(Y=Y)\n",
    "\n",
    "linear = Activation(\"linear\", 'mse')\n",
    "dA = np.array([[1.2, -0.5, 0.8, -0.3], [0.4, 0.6, -0.9, 0.2], [-0.1, 0.5, -0.7, 0.9]])\n",
    "dZ = linear.backward(dA=dA)\n",
    "print(\"Linear: dZ = \\n\" + str(dZ))\n",
    "outputs[\"linear_backward\"] = dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyyX_xxdEmNp"
   },
   "source": [
    "Expected output:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>Sigmoid: A</td>\n",
    "    <td>[[0.00669285] [0.26894142] [0.5] [0.73105858] [0.99330715]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>ReLU: A</td>\n",
    "    <td>[[0] [0] [0] [1] [5]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Softmax: A</td>\n",
    "    <td>\n",
    "      [[0.0320586 0.08714432 0.23688282 0.64391426]\n",
    "       [0.1748777 0.47536689 0.1748777 0.1748777]\n",
    "       [0.0320586 0.08714432 0.23688282 0.64391426]]\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Linear: A</td>\n",
    "    <td>\n",
    "      [[1 2 3 4]\n",
    "       [0 1 0 0]\n",
    "       [-2 -1 0 1]]\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>(with Sigmoid) dZ</td>\n",
    "    <td>[[-0.5] [-0.26935835] [-0.11969269] [-0.5] [-0.73139639]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>(with ReLU) dZ</td>\n",
    "    <td>[[0 1.7] [0 0] [-1.14 3.72]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>(with Softmax) dZ</td>\n",
    "    <td>\n",
    "      [[-0.96488097 0.70538451 0.25949646]\n",
    "       [0.09003057 -0.75527153 0.66524096]\n",
    "       [0.01766842 0.01766842 -0.03533684]]\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>(with Linear) dZ</td>\n",
    "    <td>\n",
    "      [[1.2 -0.5 0.8 -0.3]\n",
    "       [0.4 0.6 -0.9 0.2]\n",
    "       [-0.1 0.5 -0.7 0.9]]\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9vcTYp_yoPu"
   },
   "source": [
    "## Part 3: Model (10%)\n",
    "\n",
    "Use the functions that you had previously written to implement the complete neural network model, including initialization, forward propagation, backward propagation, and parameter updates.\n",
    "\n",
    "> ### Step 1: Model Initialization (0%)\n",
    "Initialize the model by creating linear and activation function layers.\n",
    "\n",
    ">> #### Requirements:\n",
    "- Store linear layers in a list called `linear`\n",
    "- Store activation function layers in a list called `activation`\n",
    "- Use iteration number as seed for each Dense layer initialization\n",
    "\n",
    ">> #### Note:\n",
    "A linear-activation pair counts as a single layer in the neural network.\n",
    "\n",
    "> ### Step 2: Forward Propagation (4%)\n",
    "Implement the model's forward pass by calling each layer's forward function sequentially.\n",
    "\n",
    ">> #### Process:\n",
    "1. For layers 1 to N-1: [LINEAR -> ACTIVATION]\n",
    "2. Final layer: LINEAR -> SIGMOID (binary) or SOFTMAX (multi-class)\n",
    "\n",
    ">> #### Note:\n",
    "For binary classification, use one output node with sigmoid activation. For K-class classification, use K output nodes with softmax activation.\n",
    "\n",
    "> ### Step 3: Backward Propagation (4%)\n",
    "Implement the model's backward pass by calling each layer's backward function in reverse order.\n",
    "\n",
    ">> #### Process:\n",
    "1. Initialize backpropagation:\n",
    "   - Regression:\n",
    "     $$dAL = AL - Y$$\n",
    "   - Binary classification:\n",
    "     $$dAL = - (\\frac{Y}{AL + \\epsilon} - \\frac{1 - Y}{1 - AL + \\epsilon})$$\n",
    "     where $\\epsilon = 10^{-5}$ to prevent division by zero\n",
    "   - Multi-class classification:\n",
    "     Use `softmax_backward` function\n",
    "2. Backpropagate through layers L to 1\n",
    "\n",
    ">> #### Note:\n",
    "Use cached values from the forward pass in each layer's backward function.\n",
    "\n",
    "> ### Step 4: Parameter Update (2%)\n",
    "Update model parameters using gradient descent.\n",
    "\n",
    ">> #### Update Rule:\n",
    "For each layer $l = 1, 2, ..., L$:\n",
    "$$W^{[l]} = W^{[l]} - \\alpha \\cdot dW^{[l]}$$\n",
    "$$b^{[l]} = b^{[l]} - \\alpha \\cdot db^{[l]}$$\n",
    "where $\\alpha$ is the learning rate\n",
    "\n",
    "This revised structure provides a clear, step-by-step breakdown of the model implementation process, mirroring the format used in Part 2. It covers all the essential components while maintaining a concise and logical flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0JGMzfIDCSVz"
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, units, activation_functions, loss_function):\n",
    "        self.units = units\n",
    "        self.activation_functions = activation_functions\n",
    "        self.loss_function = loss_function\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize layers of the neural network\n",
    "\n",
    "        Arguments:\n",
    "            self.units -- array defining network structure (e.g., [4,4,1]):\n",
    "                - Input layer: 4 nodes\n",
    "                - Hidden layer: 4 nodes\n",
    "                - Output layer: 1 node\n",
    "            self.activation_functions -- activation function for each layer (e.g., [\"relu\",\"sigmoid\"]):\n",
    "                - First layer uses ReLU\n",
    "                - Second layer uses Sigmoid\n",
    "            self.loss_function -- loss function type: \"cross_entropy\" or \"mse\"\n",
    "        \"\"\"\n",
    "        self.linear = []        # Store all Dense layers (weights & biases)\n",
    "        self.activation = []    # Store all activation function layers\n",
    "\n",
    "        for i in range(len(self.units)-1):\n",
    "            dense = Dense(self.units[i], self.units[i+1], i)\n",
    "            self.linear.append(dense)\n",
    "\n",
    "        for i in range(len(self.activation_functions)):\n",
    "            self.activation.append(Activation(self.activation_functions[i], self.loss_function))\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation through the network\n",
    "\n",
    "        Arguments:\n",
    "        X -- input data: shape (n, f)\n",
    "        Returns:\n",
    "        A -- model output:\n",
    "            - For binary classification: probability (0-1)\n",
    "            - For multi-class: probability distribution across classes\n",
    "            - For regression: predicted values\n",
    "        \"\"\"\n",
    "        A = X\n",
    "\n",
    "        # GRADED FUNCTION: model_forward\n",
    "        ### START CODE HERE ###\n",
    "        for i in range(len(self.linear)):\n",
    "            Z = self.linear[i].forward(A)\n",
    "            A = self.activation[i].forward(Z)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return A\n",
    "\n",
    "    def backward(self, AL=None, Y=None):\n",
    "        \"\"\"\n",
    "        Backward propagation to compute gradients\n",
    "\n",
    "        Arguments:\n",
    "            AL -- model output from forward propagation:\n",
    "                - For binary: probability (n,1)\n",
    "                - For multi-class: probabilities (n,C)\n",
    "            Y -- true labels:\n",
    "                - For binary: 0/1 labels (n,1)\n",
    "                - For multi-class: one-hot vectors (n,C)\n",
    "                - For regression: true values (n,1)\n",
    "\n",
    "        Returns:\n",
    "            dA_prev -- gradients for previous layer's activation\n",
    "        \"\"\"\n",
    "\n",
    "        L = len(self.linear)\n",
    "        C = Y.shape[1]\n",
    "\n",
    "        # assertions\n",
    "        warning = 'Warning: only the following 3 combinations are allowed! \\n \\\n",
    "                    1. binary classification: sigmoid + cross_entropy \\n \\\n",
    "                    2. multi-class classification: softmax + cross_entropy \\n \\\n",
    "                    3. regression: linear + mse'\n",
    "        assert self.loss_function in [\"cross_entropy\", \"mse\"], \"you're using undefined loss function!\"\n",
    "        if self.loss_function == \"cross_entropy\":\n",
    "            if Y.shape[1] == 1:  # binary classification\n",
    "                assert self.activation_functions[-1] == 'sigmoid', warning\n",
    "            else:  # multi-class classification\n",
    "                assert self.activation_functions[-1] == 'softmax', warning\n",
    "                assert self.units[-1] == Y.shape[1], f\"you should set last dim to {Y.shape[1]}(the number of classes) in multi-class classification!\"\n",
    "        elif self.loss_function == \"mse\":\n",
    "            assert self.activation_functions[-1] == 'linear', warning\n",
    "            assert self.units[-1] == Y.shape[1], \"output dimension mismatch for regression!\"\n",
    "\n",
    "        # GRADED FUNCTION: model_backward\n",
    "        ### START CODE HERE ###\n",
    "        if self.activation_functions[-1] == \"linear\":\n",
    "            # Initializing the backpropagation\n",
    "            dAL = AL - Y\n",
    "            # Lth layer (LINEAR) gradients. Inputs: \"dAL\". Outputs: \"dA_prev\"\n",
    "            dZ = self.activation[-1].backward(dA=dAL)\n",
    "            dA_prev = self.linear[-1].backward(dZ)\n",
    "\n",
    "        elif self.activation_functions[-1] == \"sigmoid\":\n",
    "            # Initializing the backpropagation\n",
    "            dAL = -(Y / (AL + 1e-5) - (1 - Y) / (1 - AL + 1e-5))\n",
    "\n",
    "            # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL\". Outputs: \"dA_prev\"\n",
    "            dZ = self.activation[-1].backward(dA=dAL)\n",
    "            dA_prev = self.linear[-1].backward(dZ)\n",
    "\n",
    "        elif self.activation_functions[-1] == \"softmax\":\n",
    "            # Initializing the backpropagation\n",
    "            dZ = self.activation[-1].backward(Y = Y)\n",
    "\n",
    "            # Lth layer (LINEAR) gradients. Inputs: \"dZ\". Outputs: \"dA_prev\"\n",
    "            dA_prev = self.linear[-1].backward(dZ)\n",
    "\n",
    "        # Loop from l=L-2 to l=0\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"dA_prev\". Outputs: \"dA_prev\"\n",
    "        for l in range(L-2, -1, -1):\n",
    "            dZ = self.activation[l].backward(dA=dA_prev, Y=Y)\n",
    "            dA_prev = self.linear[l].backward(dZ)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return dA_prev\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        learning_rate -- step size\n",
    "        \"\"\"\n",
    "\n",
    "        L = len(self.linear)\n",
    "\n",
    "        # GRADED FUNCTION: model_update_parameters\n",
    "        ### START CODE HERE ###\n",
    "        for i in range(L):\n",
    "            self.linear[i].update(learning_rate)\n",
    "        ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxQtZMmA1SNc"
   },
   "source": [
    "### Test your **Model class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EGY7_1bjcm-c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:  [[ 0.09762701  0.08976637 -0.12482558]\n",
      " [ 0.43037873 -0.1526904   0.783546  ]\n",
      " [ 0.20552675  0.29178823  0.92732552]] \n",
      "b1:  [[0. 0. 0.]]\n",
      "W2:  [[-0.20325375]\n",
      " [ 0.53968259]\n",
      " [-1.22446471]] \n",
      "b2:  [[0.]]\n",
      "With sigmoid: A = [[0.64565631]\n",
      " [0.20915937]\n",
      " [0.77902611]]\n",
      "With ReLU: A = [[0.6 ]\n",
      " [0.  ]\n",
      " [1.26]]\n",
      "With softmax: A = \n",
      "[[0.47535001 0.14317267 0.38147732]\n",
      " [0.05272708 0.75380161 0.19347131]\n",
      " [0.68692136 0.05526942 0.25780921]]\n",
      "AL = [[0.56058713]\n",
      " [0.55220559]\n",
      " [0.46331713]]\n",
      "Length of layers list = 2\n",
      "AL = [[0.11637212 0.08186754 0.0924809  0.09675205 0.12819411 0.09664001\n",
      "  0.08448599 0.09067641 0.1294968  0.08303407]\n",
      " [0.11413265 0.08432761 0.09365443 0.09736489 0.12404237 0.09726785\n",
      "  0.08664355 0.09207969 0.12512634 0.08536063]\n",
      " [0.09750771 0.07419482 0.08444682 0.10943351 0.09669465 0.11116299\n",
      "  0.08734059 0.12452515 0.13002144 0.08467232]]\n",
      "Length of layers list = 2\n"
     ]
    }
   ],
   "source": [
    "# Model initialize parameters\n",
    "model = Model([3, 3, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
    "print(\"W1: \", model.linear[0].parameters[\"W\"], \"\\nb1: \", model.linear[0].parameters[\"b\"])\n",
    "print(\"W2: \", model.linear[1].parameters[\"W\"], \"\\nb2: \", model.linear[1].parameters[\"b\"])\n",
    "\n",
    "# Model forward\n",
    "A_prev, W, b = np.array([[0.1, 1.1, 2.9],[-1.2, 0.2, -2.5],[1.9, 2.3, 3.7]]), np.array([[0.1], [0.2], [0.3]]), np.array([[-0.5]])\n",
    "model = Model([3, 1], [\"sigmoid\"], \"cross_entropy\")\n",
    "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
    "A = model.forward(A_prev)\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "A_prev, W, b = np.array([[4.35, -5.67], [-7.89, 8.12]]), np.array([[-3.54], [-2.34]]), np.array([[0.8]])\n",
    "model = Model([2, 1], [\"sigmoid\"], \"cross_entropy\")\n",
    "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
    "A = model.forward(A_prev)\n",
    "outputs[\"model_forward_sigmoid\"] = (A, (model.linear[0].cache, model.activation[0].cache))\n",
    "\n",
    "A_prev, W, b = np.array([[0.1, 1.1, 2.9],[-1.2, 0.2, -2.5],[1.9, 2.3, 3.7]]), np.array([[0.1], [0.2], [0.3]]), np.array([[-0.5]])\n",
    "model = Model([3, 1], [\"relu\"], \"cross_entropy\")\n",
    "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
    "A = model.forward(A_prev)\n",
    "print(\"With ReLU: A = \" + str(A))\n",
    "A_prev, W, b = np.array([[7.23, -4.56], [5.67, -8.90]]), np.array([[-9.12], [3.45]]), np.array([[0.25]])\n",
    "model = Model([2, 1], [\"relu\"], \"cross_entropy\")\n",
    "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
    "A = model.forward(A_prev)\n",
    "outputs[\"model_forward_relu\"] = (A, (model.linear[0].cache, model.activation[0].cache))\n",
    "\n",
    "A_prev, W, b = np.array([[0.1, 1.1, 2.9],[-1.2, 0.2, -2.5],[1.9, 2.3, 3.7]]), np.array([[0.1, -0.1, -0.1],[0.2, -0.2, 0.],[0.3, -0.3, 0.1]]), np.array([[-0.5, 0.5, 0.1]])\n",
    "model = Model([3, 3], [\"softmax\"], \"cross_entropy\")\n",
    "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
    "A = model.forward(A_prev)\n",
    "print(\"With softmax: A = \\n\" + str(A))\n",
    "A_prev, W, b = np.array([[-5.12, 4.56, 7.89], [8.34, -6.78, 2.45], [3.21, -4.67, 5.98]]), np.array([[6.23, -7.85, 4.56], [-3.21, 9.87, -2.34], [1.23, -5.67, 8.90]]), np.array([[4.12, -6.54, 7.89]])\n",
    "model = Model([3, 3], [\"softmax\"], \"cross_entropy\")\n",
    "model.linear[0].parameters = {\"W\": W, \"b\": b}\n",
    "A = model.forward(A_prev)\n",
    "outputs[\"model_forward_softmax\"] = (A, (model.linear[0].cache, model.activation[0].cache))\n",
    "\n",
    "# binary classification\n",
    "X = np.array([[0, -2, 0.5], [1, -1, 0.5], [2, 0, 0.5]])\n",
    "model = Model([3, 3, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
    "AL = model.forward(X)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of layers list = \" + str(len(model.linear)))\n",
    "\n",
    "# multi-class classification\n",
    "X = np.array([[0, -2, 0.5], [1, -1, 0.5], [2, 0, 0.5]])\n",
    "model = Model([3, 3, 10], [\"relu\", \"softmax\"], \"cross_entropy\")\n",
    "AL = model.forward(X)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of layers list = \" + str(len(model.linear)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEmggOxtdMnl"
   },
   "source": [
    "Expected output:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>W1:</td>\n",
    "    <td>[[ 0.09762701 0.08976637 -0.12482558] [ 0.43037873 -0.1526904 0.783546 ] [ 0.20552675 0.29178823 0.92732552]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>b1:</td>\n",
    "    <td>[[0. 0. 0.]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>W2:</td>\n",
    "    <td>[[-0.20325375] [ 0.53968259] [-1.22446471]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>b2:</td>\n",
    "    <td>[[0.]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>With Sigmoid:</td>\n",
    "    <td>A = [[0.64565631] [0.20915937] [0.77902611]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>With ReLU:</td>\n",
    "    <td>A = [[0.6 ] [0. ] [1.26]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>With Softmax:</td>\n",
    "    <td>A = [[0.47535001 0.14317267 0.38147732] [0.05272708 0.75380161 0.19347131] [0.68692136 0.05526942 0.25780921]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>AL:</td>\n",
    "    <td>[[0.56058713] [0.55220559] [0.46331713]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Length of layers list:</td>\n",
    "    <td>2</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>AL:</td>\n",
    "    <td>[[0.11637212 0.08186754 0.0924809  0.09675205 0.12819411 0.09664001 0.08448599 0.09067641 0.1294968  0.08303407]\n",
    "         [0.11413265 0.08432761 0.09365443 0.09736489 0.12404237 0.09726785 0.08664355 0.09207969 0.12512634 0.08536063]\n",
    "         [0.09750771 0.07419482 0.08444682 0.10943351 0.09669465 0.11116299 0.08734059 0.12452515 0.13002144 0.08467232]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Length of layers list:</td>\n",
    "    <td>2</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HOGsyLXPNGh5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.55554938  0.27777469]\n",
      " [ 0.49152369  0.24576184]\n",
      " [-0.41996594 -0.20998297]\n",
      " [-0.55554938 -0.27777469]\n",
      " [-0.39321993 -0.19660997]]\n",
      "dW = [[-0.29446117]\n",
      " [ 0.29446117]]\n",
      "db = [[-0.03216622]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[-0.01269296 -0.05595562]\n",
      " [ 0.01470136  0.06480946]\n",
      " [ 0.          0.        ]\n",
      " [-0.07496777 -0.0327431 ]\n",
      " [-0.07151883 -0.03123674]]\n",
      "dW = [[ 0.0178719  -0.17321413]\n",
      " [-0.0178719   0.17321413]]\n",
      "db = [[ 0.00335943 -0.11638953]]\n",
      "\n",
      "Binary classification\n",
      "dW1 = [[-0.06277946  0.26602938 -0.37820327]\n",
      " [ 0.          0.05875647  0.        ]\n",
      " [-0.01569486  0.05181823 -0.09455082]]\n",
      "db1 = [[-0.03138973  0.10363646 -0.18910163]]\n",
      "dA_prev = [[-0.02128713  0.03620889 -0.06919444]\n",
      " [ 0.02675119 -0.04550313  0.08695554]\n",
      " [ 0.08406585 -0.52321654 -0.47247201]]\n",
      "\n",
      "Multi-class classification\n",
      "dW1 = [[ 0.16593371  0.33171007 -0.32297709]\n",
      " [ 0.          0.15006987  0.        ]\n",
      " [ 0.04148343  0.04541005 -0.08074427]]\n",
      "db1 = [[ 0.08296685  0.0908201  -0.16148854]]\n",
      "dA_prev = [[-0.04735391  0.08054785 -0.15392528]\n",
      " [ 0.05429414 -0.09235301  0.1764847 ]\n",
      " [ 0.10229066 -0.30227651 -0.34116033]]\n",
      "\n",
      "Regression\n",
      "dW1 = [[ 0.45352627 -1.49031638  2.73218534]\n",
      " [ 0.          1.09795245  0.        ]\n",
      " [ 0.11338157 -0.64706721  0.68304634]]\n",
      "db1 = [[ 0.22676313 -1.29413441  1.36609267]]\n",
      "dA_prev = [[-0.10931473  0.18594169 -0.35533076]\n",
      " [-0.07704814  0.13105702 -0.25044727]\n",
      " [-0.60730166  3.77977844  3.41319394]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model backward\n",
    "AL, Y, linear_activation_cache = np.array([[0.1], [0.2], [0.5], [0.9], [1.0]]), np.array([[0], [0], [1], [1], [1]]), (((np.array([[-2, 2], [-1, 1], [0, 0], [1, -1], [2, -2]]), np.array([[2.0], [1.0]]), np.array([[0.5]])), np.array([[0], [1], [2], [0], [1]])))\n",
    "model = Model([2, 1], [\"sigmoid\"], \"cross_entropy\")\n",
    "model.linear[0].cache = linear_activation_cache[0]\n",
    "model.activation[0].cache = linear_activation_cache[1]\n",
    "dA_prev = model.backward(AL=AL, Y=Y)\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(model.linear[0].dW))\n",
    "print (\"db = \" + str(model.linear[0].db) + \"\\n\")\n",
    "AL, Y, linear_activation_cache = np.array([[0.35], [0.93], [0.23], [0.72], [0.90]]), np.array([[1], [0], [1], [0], [1]]), (((np.array([[-1, 2], [1, 3], [2, 0], [1, -4], [3, -2]]), np.array([[1.7], [3.2]]), np.array([[0.25]])), np.array([[2], [1], [2], [0], [0]])))\n",
    "model = Model([2, 1], [\"sigmoid\"], \"cross_entropy\")\n",
    "model.linear[0].cache = linear_activation_cache[0]\n",
    "model.activation[0].cache = linear_activation_cache[1]\n",
    "dA_prev = model.backward(AL=AL, Y=Y)\n",
    "outputs[\"model_backward_sigmoid\"] = (dA_prev, model.linear[0].dW, model.linear[0].db)\n",
    "\n",
    "X, Y = np.array([[-2, 2], [-1, 1], [0, 0], [1, -1], [2, -2]]), np.array([[0], [1], [1], [1], [1]])\n",
    "model = Model([2, 2, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
    "AL = model.forward(X)\n",
    "dA_prev = model.backward(AL=AL, Y=Y)\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(model.linear[0].dW))\n",
    "print (\"db = \" + str(model.linear[0].db) + \"\\n\")\n",
    "X, Y = np.array([[4.56, -3.21], [-7.85, 6.34], [2.45, -8.90], [5.67, 3.12], [-4.78, 7.89]]), np.array([[1], [1], [0], [1], [0]])\n",
    "model = Model([2, 2, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
    "AL = model.forward(X)\n",
    "dA_prev = model.backward(AL=AL, Y=Y)\n",
    "outputs[\"model_backward_relu\"] = (dA_prev, model.linear[0].dW, model.linear[0].db)\n",
    "\n",
    "# binary classification\n",
    "X, Y = np.array([[0, -2, 0.5], [1, -1, 0.5], [2, 0, 0.5]]), np.array([[1], [0], [0]])\n",
    "model = Model([3, 3, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
    "AL = model.forward(X)\n",
    "dA_prev = model.backward(AL=AL, Y=Y)\n",
    "print(\"Binary classification\")\n",
    "print(\"dW1 = \"+ str(model.linear[0].dW))\n",
    "print(\"db1 = \"+ str(model.linear[0].db))\n",
    "print(\"dA_prev = \"+ str(dA_prev) +\"\\n\")\n",
    "\n",
    "# multi-class classification\n",
    "X, Y = np.array([[0, -2, 0.5], [1, -1, 0.5], [2, 0, 0.5]]), np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "model = Model([3, 3, 3], [\"relu\", \"softmax\"], \"cross_entropy\")\n",
    "AL = model.forward(X)\n",
    "dA_prev = model.backward(AL=AL, Y=Y)\n",
    "print(\"Multi-class classification\")\n",
    "print(\"dW1 = \"+ str(model.linear[0].dW))\n",
    "print(\"db1 = \"+ str(model.linear[0].db))\n",
    "print(\"dA_prev = \"+ str(dA_prev) +\"\\n\")\n",
    "\n",
    "# regression - mse\n",
    "X, Y = np.array([[0, -2, 0.5], [1, -1, 0.5], [2, 0, 0.5]]), np.array([[2.5], [1.8], [3.2]])\n",
    "model = Model([3, 3, 1], [\"relu\", \"linear\"], \"mse\")\n",
    "AL = model.forward(X)\n",
    "dA_prev = model.backward(AL=AL, Y=Y)\n",
    "print(\"Regression\")\n",
    "print(\"dW1 = \"+ str(model.linear[0].dW))\n",
    "print(\"db1 = \"+ str(model.linear[0].db))\n",
    "print(\"dA_prev = \"+ str(dA_prev) +\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6xzEk3-NGh6"
   },
   "source": [
    "Expected output:\n",
    "<table>\n",
    "  <tr>\n",
    "    <th colspan=\"2\">Sigmoid</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dA_prev:</td>\n",
    "    <td>[[ 0.55554938  0.27777469] [ 0.49152369  0.24576184] [-0.41996594 -0.20998297] [-0.55554938 -0.27777469] [-0.39321993 -0.19660997]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dW:</td>\n",
    "    <td>[[-0.29446117] [ 0.29446117]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>db:</td>\n",
    "    <td>[[-0.03216622]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th colspan=\"2\">ReLU</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dA_prev:</td>\n",
    "    <td>[[-0.01269296 -0.05595562] [ 0.01470136  0.06480946] [ 0.  0. ] [-0.07496777 -0.0327431 ] [-0.07151883 -0.03123674]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dW:</td>\n",
    "    <td>[[ 0.0178719  -0.17321413] [-0.0178719   0.17321413]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>db:</td>\n",
    "    <td>[[ 0.00335943 -0.11638953]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th colspan=\"2\">Binary Classification</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dW1:</td>\n",
    "    <td>[[-0.06277946  0.26602938 -0.37820327] [ 0.  0.05875647  0. ] [-0.01569486  0.05181823 -0.09455082]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>db1:</td>\n",
    "    <td>[[-0.03138973  0.10363646 -0.18910163]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dA_prev:</td>\n",
    "    <td>[[-0.02128713  0.03620889 -0.06919444] [ 0.02675119 -0.04550313  0.08695554] [ 0.08406585 -0.52321654 -0.47247201]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th colspan=\"2\">Multi-class Classification</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dW1:</td>\n",
    "    <td>[[ 0.16593371  0.33171007 -0.32297709] [ 0.  0.15006987  0. ] [ 0.04148343  0.04541005 -0.08074427]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>db1:</td>\n",
    "    <td>[[ 0.08296685  0.0908201  -0.16148854]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dA_prev:</td>\n",
    "    <td>[[-0.04735391  0.08054785 -0.15392528] [ 0.05429414 -0.09235301  0.1764847 ] [ 0.10229066 -0.30227651 -0.34116033]]</td>\n",
    "  </tr>\n",
    "  <th colspan=\"2\">Regression</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dW1:</td>\n",
    "    <td>[[ 0.45352627 -1.49031638  2.73218534] [ 0.          1.09795245  0.        ] [ 0.11338157 -0.64706721  0.68304634]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>db1:</td>\n",
    "    <td>[[ 0.22676313 -1.29413441  1.36609267]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>dA_prev:</td>\n",
    "    <td>[[-0.10931473  0.18594169 -0.35533076] [-0.07704814  0.13105702 -0.25044727] [-0.60730166  3.77977844  3.41319394]]</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qoGA4O8BUCvq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.39721186  0.07752363  0.392862  ]\n",
      " [ 0.64025004  0.00469968  0.52183369]\n",
      " [-0.09671178  0.09679955  0.33138026]\n",
      " [ 0.27099015  0.33705631  0.67538482]]\n",
      "b1 = [[ 0.16234149  0.78232848 -0.02592894]]\n",
      "W2 = [[0.6012798 ]\n",
      " [0.38575324]\n",
      " [0.49003974]]\n",
      "b2 = [[0.05692437]]\n"
     ]
    }
   ],
   "source": [
    "# Model update\n",
    "np.random.seed(1)\n",
    "parameters, grads = {\"W1\": np.random.rand(3, 4).T, \"b1\": np.random.rand(3,1).T, \"W2\": np.random.rand(1,3).T, \"b2\": np.random.rand(1,1).T}, {\"dW1\": np.random.rand(3, 4).T, \"db1\": np.random.rand(3,1).T, \"dW2\": np.random.rand(1,3).T, \"db2\": np.random.rand(1,1).T}\n",
    "model = Model([4, 3, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
    "model.linear[0].parameters = {\"W\": parameters[\"W1\"], \"b\": parameters[\"b1\"]}\n",
    "model.linear[1].parameters = {\"W\": parameters[\"W2\"], \"b\": parameters[\"b2\"]}\n",
    "model.linear[0].dW, model.linear[0].db, model.linear[1].dW, model.linear[1].db = grads[\"dW1\"], grads[\"db1\"], grads[\"dW2\"], grads[\"db2\"]\n",
    "model.update(0.1)\n",
    "print (\"W1 = \"+ str(model.linear[0].parameters[\"W\"]))\n",
    "print (\"b1 = \"+ str(model.linear[0].parameters[\"b\"]))\n",
    "print (\"W2 = \"+ str(model.linear[1].parameters[\"W\"]))\n",
    "print (\"b2 = \"+ str(model.linear[1].parameters[\"b\"]))\n",
    "\n",
    "np.random.seed(1)\n",
    "parameters, grads = {\"W1\": np.random.rand(3, 4).T, \"b1\": np.random.rand(3,1).T, \"W2\": np.random.rand(1,3).T, \"b2\": np.random.rand(1,1).T}, {\"dW1\": np.random.rand(3, 4).T, \"db1\": np.random.rand(3,1).T, \"dW2\": np.random.rand(1,3).T, \"db2\": np.random.rand(1,1).T}\n",
    "model = Model([4, 3, 1], [\"relu\", \"sigmoid\"], \"cross_entropy\")\n",
    "model.linear[0].parameters = {\"W\": parameters[\"W1\"], \"b\": parameters[\"b1\"]}\n",
    "model.linear[1].parameters = {\"W\": parameters[\"W2\"], \"b\": parameters[\"b2\"]}\n",
    "model.linear[0].dW, model.linear[0].db, model.linear[1].dW, model.linear[1].db = grads[\"dW1\"], grads[\"db1\"], grads[\"dW2\"], grads[\"db2\"]\n",
    "model.update(0.075)\n",
    "outputs[\"model_update_parameters\"] = {\"W1\": model.linear[0].parameters[\"W\"], \"b1\": model.linear[0].parameters[\"b\"], \"W2\": model.linear[1].parameters[\"W\"], \"b2\": model.linear[1].parameters[\"b\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9t-HfnHZWYIa"
   },
   "source": [
    "Expected output:\n",
    "<table>\n",
    "  <tr>\n",
    "    <th colspan=\"2\">Data Representation</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>W1:</td>\n",
    "    <td>[[ 0.39721186  0.07752363  0.392862 ] [ 0.64025004  0.00469968  0.52183369] [-0.09671178  0.09679955  0.33138026] [ 0.27099015  0.33705631  0.67538482]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>b1:</td>\n",
    "    <td>[[ 0.16234149  0.78232848 -0.02592894]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>W2:</td>\n",
    "    <td>[[0.6012798 ] [0.38575324] [0.49003974]]</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>b2:</td>\n",
    "    <td>[[0.05692437]]</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmSBVaQOSRrk"
   },
   "source": [
    "## **Section 2: Loss function(10%)**\n",
    "In this section, you need to implement the loss function. We use binary cross-entropy loss for binary classification and categorical cross-entropy loss for multi-class classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ScdQdj85uC0P"
   },
   "source": [
    "## Part 1: Binary cross-entropy loss (BCE) (5%)\n",
    "Compute the binary cross-entropy loss $L$, using the following formula:  $$-\\frac{1}{n} \\sum\\limits_{i = 1}^{n} (y^{(i)}\\log\\left(a^{[L] (i)}+\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}+\\right)), where\\ =1e-5$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MjBT0eYQaY81"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_BCE_loss\n",
    "\n",
    "def compute_BCE_loss(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the binary cross-entropy loss function using the above formula.\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (n, 1)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (n, 1)\n",
    "\n",
    "    Returns:\n",
    "    loss -- binary cross-entropy loss\n",
    "    \"\"\"\n",
    "\n",
    "    n = Y.shape[0]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### ( 1 line of code)\n",
    "    loss = -(np.sum(Y * np.log(AL + 1e-5) + (1 - Y) * np.log(1 - AL + 1e-5))) / n\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    loss = np.squeeze(loss)      # To make sure your loss's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(loss.shape == ())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoV03IzimBEN"
   },
   "source": [
    "### Test your **compute_BCE_loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "r07sqnIXaaMv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.5783820772863568\n"
     ]
    }
   ],
   "source": [
    "AL, Y = np.array([[0.9], [0.6], [0.4], [0.1], [0.2], [0.8]]), np.array([[1], [1], [1], [0], [0], [0]])\n",
    "\n",
    "print(\"loss = \" + str(compute_BCE_loss(AL, Y)))\n",
    "outputs[\"compute_BCE_loss\"] = compute_BCE_loss(np.array([[0.12], [0.85], [0.47], [0.33], [0.76], [0.58], [0.09], [0.62]]), np.array([[1], [1], [0], [1], [0], [1], [1], [0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iRtgOx_IGPo"
   },
   "source": [
    "Expected output:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>loss: </td>\n",
    "    <td>0.5783820772863568</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aealRyKbcQzG"
   },
   "source": [
    "## Part 2: Categorical cross-entropy loss (CCE) (5%)\n",
    "Compute the categorical cross-entropy loss $L$, using the following formula: $$-\\frac{1}{n} \\sum\\limits_{i = 1}^{n} (y^{(i)}\\log\\left(a^{[L] (i)}+\\right)),\\  = 1e-5$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Owx-kTdcfxV5"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_CCE_loss\n",
    "\n",
    "def compute_CCE_loss(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the categorical cross-entropy loss function using the above formula.\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (n, C)\n",
    "    Y -- true \"label\" vector (one hot vector, for example: [1,0,0] represents rock, [0,1,0] represents paper, [0,0,1] represents scissors\n",
    "                                      in a Rock-Paper-Scissors, shape: (n, C)\n",
    "\n",
    "    Returns:\n",
    "    loss -- categorical cross-entropy loss\n",
    "    \"\"\"\n",
    "\n",
    "    n = Y.shape[0]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### ( 1 line of code)\n",
    "    loss = -(np.sum(Y * np.log(AL + 1e-5))) / n\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    loss = np.squeeze(loss)      # To make sure your loss's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(loss.shape == ())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSOsacYQmNAb"
   },
   "source": [
    "### Test your **compute_CCE_loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "0YbHVAc7hSh3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.4722526144672341\n"
     ]
    }
   ],
   "source": [
    "AL, Y = np.array([[0.8, 0.1, 0.1],[0.6, 0.3, 0.1],[0.4, 0.5, 0.1],[0.1, 0.7, 0.2],[0.2, 0.1, 0.7],[0.4, 0.1, 0.5]]), np.array([[1, 0, 0],[1, 0, 0],[0, 1, 0],[0, 1, 0],[0, 0, 1],[0, 0, 1]])\n",
    "print(\"loss = \" + str(compute_CCE_loss(AL, Y)))\n",
    "outputs[\"compute_CCE_loss\"] = compute_CCE_loss(np.array([[0.7, 0.2, 0.1], [0.2, 0.2, 0.6], [0.3, 0.5, 0.2], [0.8, 0.1, 0.1], [0.7, 0.15, 0.15]]), np.array([[1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 0, 1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9VVIBB5Ic-D"
   },
   "source": [
    "Expected output:\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>loss: </td>\n",
    "    <td>0.4722526144672341</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_XIpJtBpiAX"
   },
   "source": [
    "## Part 3: Mean square error (MSE) (0%)\n",
    "You don't need to write this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6RHLNGsepygt"
   },
   "outputs": [],
   "source": [
    "# compute_MSE_loss (MSE)\n",
    "def compute_MSE_loss(AL, Y):\n",
    "    m = Y.shape[0]\n",
    "    loss = (1/m) * np.sum(np.square(AL - Y))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8_AYhzfqlbg"
   },
   "source": [
    "## **Section 3: Training and prediction(35%)**\n",
    "In this section, you will apply your implemented neural network to regression and binary classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpFQpiK5eF64"
   },
   "source": [
    "## Helper function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "woCqucFUYXe6"
   },
   "outputs": [],
   "source": [
    "def predict(x, y_true, model):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "\n",
    "    Arguments:\n",
    "    x -- data set of examples you would like to label\n",
    "    model -- trained model\n",
    "\n",
    "    Returns:\n",
    "    y_pred -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "\n",
    "    n = x.shape[0]\n",
    "\n",
    "    # Forward propagation\n",
    "    y_pred = model.forward(x)\n",
    "\n",
    "    # this transform the output and label of binary classification when using sigmoid + cross entropy for evaluation\n",
    "    # eg. y_pred: [[0.8], [0.2], [0.1]] -> [[0.2, 0.8], [0.8, 0.2], [0.9, 0.1]]\n",
    "    # eg. y_true: [[1], [0], [0]] -> [[0, 1], [1, 0], [1, 0]]\n",
    "    if y_pred.shape[-1] == 1:\n",
    "        y_pred = np.array([[1 - y[0], y[0]] for y in y_pred])\n",
    "        if y_true is not None:\n",
    "            y_true = np.array([[1,0] if y == 0 else [0,1] for y in y_true.reshape(-1)])\n",
    "\n",
    "    # make y_pred/y_true become one-hot prediction result\n",
    "    # eg. y_true: [[1, 0, 0], [0, 0, 1], [0, 1, 0]] -> [0, 2, 1]\n",
    "    # eg. y_pred: [[0.2, 0.41, 0.39], [0.1, 0.8, 0.1], [0.1, 0.1, 0.8]] -> [1, 1, 2]\n",
    "    if y_true is not None:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    if y_true is not None:\n",
    "        # compute accuracy\n",
    "        correct = 0\n",
    "        for yt, yp in zip(y_true, y_pred):\n",
    "            if yt == yp:\n",
    "                correct += 1\n",
    "        print(f\"Accuracy: {correct/n * 100:.2f}%\")\n",
    "\n",
    "        f1_scores = f1_score(y_true, y_pred, average=None)\n",
    "        print(f'f1 score for each class: {f1_scores}')\n",
    "        print(f'f1_macro score: {np.mean(np.array(f1_scores)):.2f}')\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def save_prediction_data(predicted_y):\n",
    "    # Create DataFrame with ID, x, and y columns\n",
    "    df = pd.DataFrame({\n",
    "        'ID': range(len(predicted_y)),  # Add ID column starting from 0\n",
    "        'y': predicted_y\n",
    "    })\n",
    "\n",
    "    # Ensure ID is the first column\n",
    "    df = df[['ID', 'y']]\n",
    "\n",
    "    # Save to CSV file\n",
    "    df.to_csv('Lab4_basic_regression.csv', index=False)\n",
    "    print(\"Prediction data saved as 'Lab4_basic_regression.csv'\")\n",
    "\n",
    "def animate_training(history, X_train, Y_train):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(0, 11)\n",
    "    ax.set_ylim(-5, 5)\n",
    "    line, = ax.plot([], [], 'b-', lw=1, label='Predicted')\n",
    "\n",
    "    ground_truth_x = X_train.flatten()\n",
    "    ground_truth_y = Y_train.flatten()\n",
    "    ax.plot(ground_truth_x, ground_truth_y, 'r-', lw=1, label='Ground Truth')\n",
    "\n",
    "    # show current epoch on the animation / 100 epoch\n",
    "    epoch_text = ax.text(0.05, 0.95, '', transform=ax.transAxes, fontsize=12, verticalalignment='top')\n",
    "\n",
    "    def init():\n",
    "        line.set_data([], [])\n",
    "        epoch_text.set_text('')\n",
    "        return line, epoch_text\n",
    "\n",
    "    def update(frame):\n",
    "        epoch = (frame + 1) * 100\n",
    "        _, predicted_y = history[frame]\n",
    "        predicted_x = X_train.flatten()\n",
    "        line.set_data(predicted_x, predicted_y.flatten())\n",
    "\n",
    "        epoch_text.set_text(f'Epoch: {epoch}')\n",
    "\n",
    "        return line, epoch_text\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=len(history), init_func=init, blit=True, interval=50)\n",
    "\n",
    "    # save as gif\n",
    "    ani.save('Lab4_basic_regression.gif', writer='pillow')\n",
    "    plt.close(fig)\n",
    "    print(f\"Animation saved as 'Lab4_basic_regression.gif'\")\n",
    "\n",
    "\n",
    "def save_final_result(model, X_train, Y_train):\n",
    "    AL = model.forward(X_train)\n",
    "\n",
    "    predicted_x = X_train.flatten()\n",
    "    predicted_y = AL.flatten()\n",
    "\n",
    "    plt.plot(predicted_x, predicted_y, 'b-', label=\"Predicted\", lw=1)\n",
    "\n",
    "    ground_truth_x = X_train.flatten()\n",
    "    ground_truth_y = Y_train.flatten()\n",
    "\n",
    "    save_prediction_data(predicted_y)\n",
    "\n",
    "    plt.plot(ground_truth_x, ground_truth_y, 'r-', label='Ground Truth', lw=1)\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.ylim(-5, 5)\n",
    "    plt.xlim(0, 11)\n",
    "    plt.savefig(\"Lab4_basic_regression.jpg\")\n",
    "    plt.show()\n",
    "    print(\"Prediction saved as 'Lab4_basic_regression.jpg'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgVRVmOYG9FK"
   },
   "source": [
    "## Part1: Training function & batch function (5%)\n",
    "The functions defined in this part will be utilized in the subsequent training parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "fjOBHI0bGVE7"
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, of shape (n, f^{0})\n",
    "    Y -- true \"label\" vector, of shape (n, C)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "\n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation]\n",
    "    shuffled_Y = Y[permutation]\n",
    "\n",
    "    # Step 2 - Partition (shuffled_X, shuffled_Y).\n",
    "    # Cases with a complete mini batch size only i.e each of 64 examples.\n",
    "    num_complete_minibatches = math.floor(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : (k + 1) * mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size : (k + 1) * mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    # For handling the end case (last mini-batch < mini_batch_size i.e less than 64)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[-(m % mini_batch_size):]\n",
    "        mini_batch_Y = shuffled_Y[-(m % mini_batch_size):]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return mini_batches\n",
    "\n",
    "def train_model(model, X_train, Y_train, learning_rate, num_iterations, batch_size=None, print_loss=True, print_freq=1000, decrease_freq=100, decrease_proportion=0.99):\n",
    "    \"\"\"\n",
    "    Trains the model using mini-batch gradient descent\n",
    "\n",
    "    Arguments:\n",
    "    model -- the model to be trained\n",
    "    X_train -- training set, of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels, of shape (1, m_train)\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    batch_size -- size of a mini batch\n",
    "    print_loss -- if True, print the loss every print_freq iterations\n",
    "    print_freq -- print frequency\n",
    "    decrease_freq -- learning rate decrease frequency\n",
    "    decrease_proportion -- learning rate decrease proportion\n",
    "\n",
    "    Returns:\n",
    "    model -- the trained model\n",
    "    losses -- list of losses computed during the optimization\n",
    "    history -- list of (X_train, Y_pred) tuples for visualization\n",
    "    \"\"\"\n",
    "\n",
    "    history = []\n",
    "    losses = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        ### START CODE HERE ###\n",
    "        # Define mini batches\n",
    "        if batch_size:\n",
    "            mini_batches = random_mini_batches(X_train, Y_train, batch_size)\n",
    "        else:\n",
    "            # if batch_size is None, batch is not used, mini_batch = whole dataset\n",
    "            mini_batches = random_mini_batches(X_train, Y_train, X_train.shape[0])\n",
    "\n",
    "        epoch_loss = 0\n",
    "        for batch in mini_batches:\n",
    "            X_batch, Y_batch = batch\n",
    "\n",
    "            # Forward pass\n",
    "            AL = model.forward(X_batch)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = 0\n",
    "            if model.loss_function == 'cross_entropy':\n",
    "                if model.activation_functions[-1] == \"sigmoid\": # Binary classification\n",
    "                    loss = compute_CCE_loss(AL, Y_batch)\n",
    "                elif model.activation_functions[-1] == \"softmax\": # Multi-class classification\n",
    "                    loss = compute_CCE_loss(AL, Y_batch)\n",
    "            elif model.loss_function == 'mse': # Regression\n",
    "                loss = compute_MSE_loss(AL, Y_batch)\n",
    "            epoch_loss += loss\n",
    "\n",
    "            # Backward pass\n",
    "            model.backward(AL, Y_batch)\n",
    "\n",
    "            # Update parameters\n",
    "            model.update(learning_rate)\n",
    "\n",
    "        epoch_loss /= len(mini_batches)\n",
    "        losses.append(epoch_loss)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Print loss\n",
    "        if print_loss and i % print_freq == 0:\n",
    "            print(f\"Loss after iteration {i}: {epoch_loss}\")\n",
    "\n",
    "        # Store history\n",
    "        if i % 100 == 0:\n",
    "            history.append((X_train, model.forward(X_train)))\n",
    "\n",
    "        # Decrease learning rate\n",
    "        if i % decrease_freq == 0 and i > 0:\n",
    "            learning_rate *= decrease_proportion\n",
    "\n",
    "    return model, losses, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2D8lubAw8pX"
   },
   "source": [
    "## Part 2: Regression (10%)\n",
    "In this part, Your task is to train a neural network model to approximate the following mathematical function:\n",
    "\n",
    "$$y = sin(2 * sin(2 * sin(2 * sin(x))))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-ksy7v-Hrrt"
   },
   "source": [
    "> ### Step 1: Data generation\n",
    "Generate the mathematical function :  $$y = sin(2 * sin(2 * sin(2 * sin(x))))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "0yRO6y_FyLPM"
   },
   "outputs": [],
   "source": [
    "def generate_data(num_points=1000):\n",
    "\n",
    "    x = np.linspace(0.01, 11, num_points)\n",
    "    y = np.sin(2 * np.sin(2 * np.sin(2 * np.sin(x))))\n",
    "\n",
    "    return x.reshape(-1, 1), y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VC_BxLJJHxHD"
   },
   "source": [
    "> ### Step 2: Train model\n",
    "Implement and train your model using the generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "alsJ4F6eHZ2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 8.639614977615555\n",
      "Loss after iteration 1000: 0.7356003476657074\n",
      "Loss after iteration 2000: 0.6422295841412785\n",
      "Loss after iteration 3000: 0.5622728270241086\n",
      "Loss after iteration 4000: 0.464291845373241\n",
      "Loss after iteration 5000: 0.3008698648986104\n",
      "Loss after iteration 6000: 0.2016356621095523\n",
      "Loss after iteration 7000: 0.14897678810878373\n",
      "Loss after iteration 8000: 0.1200303854789032\n",
      "Loss after iteration 9000: 0.10476172818169456\n",
      "Loss after iteration 10000: 0.09266113380911353\n",
      "Loss after iteration 11000: 0.08447457345069459\n",
      "Loss after iteration 12000: 0.07647371831167157\n",
      "Loss after iteration 13000: 0.07610517553340408\n",
      "Loss after iteration 14000: 0.06965563211493574\n",
      "Loss after iteration 15000: 0.06919893533510228\n",
      "Loss after iteration 16000: 0.06432153673187667\n",
      "Loss after iteration 17000: 0.054815844594071975\n",
      "Loss after iteration 18000: 0.05221773025833006\n",
      "Loss after iteration 19000: 0.04800531309253124\n",
      "Loss after iteration 20000: 0.05234280248448116\n",
      "Loss after iteration 21000: 0.03400767406078221\n",
      "Loss after iteration 22000: 0.04262462578962788\n",
      "Loss after iteration 23000: 0.043775493571983765\n",
      "Loss after iteration 24000: 0.03720743088509414\n",
      "Loss after iteration 25000: 0.0380290201699681\n",
      "Loss after iteration 26000: 0.023613199181335403\n",
      "Loss after iteration 27000: 0.03269236582598464\n",
      "Loss after iteration 28000: 0.03200763867121465\n",
      "Loss after iteration 29000: 0.030556151402361034\n",
      "Loss after iteration 30000: 0.030897534796554105\n",
      "Loss after iteration 31000: 0.01480674361914924\n",
      "Loss after iteration 32000: 0.020936876437441763\n",
      "Loss after iteration 33000: 0.024066089333756355\n",
      "Loss after iteration 34000: 0.024374744781076742\n",
      "Loss after iteration 35000: 0.025755919191940622\n",
      "Loss after iteration 36000: 0.00954041035061691\n",
      "Loss after iteration 37000: 0.012983516778507873\n",
      "Loss after iteration 38000: 0.016885617025802012\n",
      "Loss after iteration 39000: 0.017343413083431486\n",
      "Loss after iteration 40000: 0.017382121111926788\n",
      "Loss after iteration 41000: 0.007107318552243298\n",
      "Loss after iteration 42000: 0.007524010144707757\n",
      "Loss after iteration 43000: 0.009257666870770263\n",
      "Loss after iteration 44000: 0.011483768153598717\n",
      "Loss after iteration 45000: 0.011896243255744433\n",
      "Loss after iteration 46000: 0.005303180845972567\n",
      "Loss after iteration 47000: 0.005042717996278828\n",
      "Loss after iteration 48000: 0.004805625280147985\n",
      "Loss after iteration 49000: 0.006440660354890083\n",
      "Loss after iteration 50000: 0.00705795984883866\n",
      "Loss after iteration 51000: 0.00422078964212478\n",
      "Loss after iteration 52000: 0.004054993683570931\n",
      "Loss after iteration 53000: 0.003901755556463151\n",
      "Loss after iteration 54000: 0.0037597196440616065\n",
      "Loss after iteration 55000: 0.003630485411274781\n",
      "Loss after iteration 56000: 0.0035131602181078655\n",
      "Loss after iteration 57000: 0.003407566125963997\n",
      "Loss after iteration 58000: 0.0033089748431371514\n",
      "Loss after iteration 59000: 0.003215960087210256\n",
      "Loss after iteration 60000: 0.0031276404140706394\n",
      "Loss after iteration 61000: 0.003052335121965488\n",
      "Loss after iteration 62000: 0.0029809606588294965\n",
      "Loss after iteration 63000: 0.002913313851898099\n",
      "Loss after iteration 64000: 0.002849680600710281\n",
      "Loss after iteration 65000: 0.002789208585921223\n",
      "Loss after iteration 66000: 0.002737195641543268\n",
      "Loss after iteration 67000: 0.00268716807449\n",
      "Loss after iteration 68000: 0.0026391697184660733\n",
      "Loss after iteration 69000: 0.002593022602127518\n",
      "Loss after iteration 70000: 0.0025485921604384886\n",
      "Loss after iteration 71000: 0.0025098375939402963\n",
      "Loss after iteration 72000: 0.002472744473754234\n",
      "Loss after iteration 73000: 0.0024372613516528406\n",
      "Loss after iteration 74000: 0.0024031102102977414\n",
      "Loss after iteration 75000: 0.002370133031119159\n",
      "Loss after iteration 76000: 0.0023413585878603563\n",
      "Loss after iteration 77000: 0.002313374360091811\n",
      "Loss after iteration 78000: 0.002286070851526106\n",
      "Loss after iteration 79000: 0.0022595075451981414\n",
      "Loss after iteration 80000: 0.0022336849218486253\n",
      "Loss after iteration 81000: 0.002210986359292439\n",
      "Loss after iteration 82000: 0.0021887991251365504\n",
      "Loss after iteration 83000: 0.002167125197791483\n",
      "Loss after iteration 84000: 0.00214590942864277\n",
      "Loss after iteration 85000: 0.0021251250639416183\n",
      "Loss after iteration 86000: 0.002106866470863884\n",
      "Loss after iteration 87000: 0.002089022862592564\n",
      "Loss after iteration 88000: 0.0020715309796547512\n",
      "Loss after iteration 89000: 0.00205436656361125\n",
      "Loss after iteration 90000: 0.0020375301327687054\n",
      "Loss after iteration 91000: 0.0020226630183371942\n",
      "Loss after iteration 92000: 0.0020080688832549514\n",
      "Loss after iteration 93000: 0.001993742285779032\n",
      "Loss after iteration 94000: 0.0019796614153909323\n",
      "Loss after iteration 95000: 0.0019658325064616\n",
      "Loss after iteration 96000: 0.00195360299265554\n",
      "Loss after iteration 97000: 0.0019415733261059264\n",
      "Loss after iteration 98000: 0.001929772455139709\n",
      "Loss after iteration 99000: 0.001918150019635125\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "x_train, y_train = generate_data()\n",
    "loss_function = \"mse\"\n",
    "layers_dims = [1, 32, 64, 64, 32, 1]\n",
    "activation_fn = ['linear', 'relu', 'relu', 'relu', 'linear']\n",
    "learning_rate = 0.01\n",
    "num_iterations = 100000\n",
    "print_loss = True\n",
    "print_freq = 1000\n",
    "decrease_freq = 5000\n",
    "decrease_proportion = 0.9\n",
    "# You don't necessarily need to use mini_batch in this part\n",
    "batch_size = None\n",
    "\n",
    "model = Model(layers_dims, activation_fn, loss_function)\n",
    "model, losses, history = train_model(model, x_train, y_train, learning_rate, num_iterations, batch_size, print_loss, print_freq, decrease_freq, decrease_proportion)\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Plot the loss\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Training Loss (Initial LR: {learning_rate})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQQtLyFgL4WD"
   },
   "source": [
    "> ### Step 3: Save prediction\n",
    "Save your model's predictions to:\n",
    "> * *Lab4_basic_regression.csv*\n",
    "> * *Lab4_basic_regression.jpg*\n",
    "> * *Lab4_basic_regression.gif*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "0uwle3uqL9Em"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction data saved as 'Lab4_basic_regression.csv'\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiIAAAGdCAYAAAAvwBgXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA6d0lEQVR4nO3deXxU9b3/8fdkJjPZZiasWSCBsEgSCMiiCKigoli9Xq2tdqEtqD9tFRREq9BeK60LarXXihbUXtG2etVbd9QqUgG17DuaBMGwLwGUmawzycz5/TEwmsqSQJJvZng9H4/zmJkzZ+Z85mRyznu+53vOsVmWZQkAAMCABNMFAACAUxdBBAAAGEMQAQAAxhBEAACAMQQRAABgDEEEAAAYQxABAADGEEQAAIAxDtMFHEs4HNauXbvkdrtls9lMlwMAABrBsixVVFQoOztbCQnHbvNo00Fk165dysnJMV0GAAA4Adu3b1fXrl2POU2bDiJut1tS5IN4PB7D1QAAgMbw+/3KycmJbsePpU0HkcO7YzweD0EEAIAY05huFXRWBQAAxhBEAACAMQQRAABgTJvuIwIAaF2WZam+vl6hUMh0KWjD7Ha7HA5Hs5xagyACAJAkBYNB7d69W9XV1aZLQQxISUlRVlaWnE7nSb0PQQQAoHA4rLKyMtntdmVnZ8vpdHIiSRyRZVkKBoPat2+fysrK1Lt37+OetOxYCCIAAAWDQYXDYeXk5CglJcV0OWjjkpOTlZiYqK1btyoYDCopKemE34vOqgCAqJP5ZYtTS3N9V/jGAQAAYwgiAADAGIIIAACNMH78eF1xxRXRx6NGjdLkyZNbvY4FCxbIZrPp4MGDrT7vlkAQAQDEtPHjx8tms8lms8npdKpXr1763e9+p/r6+had76uvvqp77rmnUdPGW3hoThw1AwCIeRdffLHmzJmjQCCgd955RxMmTFBiYqKmTZvWYLpgMHjS5704rH379s3yPqc6WkQAADHP5XIpMzNT3bp104033qjRo0frzTffjO5Oue+++5Sdna0+ffpIkrZv366rr75a6enpat++vS6//HJt2bIl+n6hUEhTpkxRenq6OnTooDvuuEOWZTWY57/vmgkEArrzzjuVk5Mjl8ulXr166X/+53+0ZcsWnXfeeZKkdu3ayWazafz48ZIi52+ZMWOG8vLylJycrAEDBujvf/97g/m88847Ou2005ScnKzzzjuvQZ3xgBYRAMBRVVdLJSWtP9/8fOlkTmeSnJysAwcOSJLmz58vj8ejefPmSZLq6uo0ZswYDRs2TB999JEcDofuvfdeXXzxxVq3bp2cTqceeeQRPfvss3rmmWdUUFCgRx55RK+99prOP//8o87zZz/7mRYvXqzHHntMAwYMUFlZmfbv36+cnBy98sor+t73vqfS0lJ5PB4lJydLkmbMmKG//e1vmj17tnr37q1FixbpJz/5iTp16qSRI0dq+/btuvLKKzVhwgTdcMMNWrFihW677bYTXzBtEEEEAHBUJSXS4MGtP9+VK6VBg5r+OsuyNH/+fL333nu6+eabtW/fPqWmpurPf/5zdJfM3/72N4XDYf35z3+Onj12zpw5Sk9P14IFC3TRRRfp0Ucf1bRp03TllVdKkmbPnq333nvvqPPduHGjXn75Zc2bN0+jR4+WJPXo0SP6/OHdOJ07d1Z6erqkSAvK/fffrw8++EDDhg2Lvubjjz/Wk08+qZEjR2rWrFnq2bOnHnnkEUlSnz59tH79ej344INNXzhtFEEEAHBU+fmRUGBivk0xd+5cpaWlqa6uTuFwWD/+8Y81ffp0TZgwQUVFRQ36haxdu1abNm2S2+1u8B61tbXavHmzfD6fdu/eraFDh0afczgcGjJkyLd2zxy2Zs0a2e12jRw5stE1b9q0SdXV1brwwgsbjA8Ggxo4cKAkqbi4uEEdkqKhJV4QRAAAR5WScmItE63tvPPO06xZs+R0OpWdnS2H4+vNW2pqaoNpKysrNXjwYD3//PPfep9OnTqd0PwP72ppisrKSknS22+/rS5dujR4zuVynVAdsYggAgCIeampqerVq1ejph00aJBeeuklde7cWR6P54jTZGVlaenSpTr33HMlSfX19Vq5cqUGHSWVFRUVKRwOa+HChdFdM990uEUmFApFxxUWFsrlcmnbtm1HbUkpKCjQm2++2WDckiVLjv8hYwhHzQAATiljx45Vx44ddfnll+ujjz5SWVmZFixYoFtuuUU7duyQJE2aNEkPPPCAXn/9dZWUlOimm2465jlAunfvrnHjxunaa6/V66+/Hn3Pl19+WZLUrVs32Ww2zZ07V/v27VNlZaXcbrduv/123XrrrXruuee0efNmrVq1SjNnztRzzz0nSfrFL36hzz//XL/85S9VWlqqF154Qc8++2xLL6JWRRABAJxSUlJStGjRIuXm5urKK69UQUGBrrvuOtXW1kZbSG677Tb99Kc/1bhx4zRs2DC53W5997vfPeb7zpo1S9///vd10003KT8/X9dff72qqqokSV26dNFvf/tbTZ06VRkZGZo4caIk6Z577tFdd92lGTNmqKCgQBdffLHefvtt5eXlSZJyc3P1yiuv6PXXX9eAAQM0e/Zs3X///S24dFqfzTpaz5s2wO/3y+v1yufzHbX5DABw8mpra1VWVqa8vLyTuqQ7Th3H+s40ZftNiwgAADCGIAIAAIwhiAAAAGMIIgAAwBiCCAAAMIYgAgAAjCGIAAAAY1otiDzwwAOy2WyaPHlya80SAAC0ca0SRJYvX64nn3xS/fv3b43ZAQAQE6ZPn67TTz/ddBmSpFGjRhlpLGjxIFJZWamxY8fq6aefVrt27Vp6dgCAU9CePXs0adIk9erVS0lJScrIyNCIESM0a9YsVVdXmy7vhEyfPl02m+2Yw4lYsGCBbDbbMa+d05paPIhMmDBBl1566RGvRggAwMn64osvNHDgQL3//vu6//77tXr1ai1evFh33HGH5s6dqw8++OCor62rq2vFSpvm9ttv1+7du6ND165d9bvf/a7BuG8KBoOGKj05LRpEXnzxRa1atUozZsxo1PSBQEB+v7/BAADAsdx0001yOBxasWKFrr76ahUUFKhHjx66/PLL9fbbb+uyyy6LTmuz2TRr1iz953/+p1JTU3XfffdJilywrmfPnnI6nerTp4/++te/Rl+zZcsW2Ww2rVmzJjru4MGDstlsWrBggaSvWxnmz5+vIUOGKCUlRcOHD1dpaWmDWh944AFlZGTI7XZHL7R3NGlpacrMzIwOdrtdbrc7+viHP/yhJk6cqMmTJ6tjx44aM2bMcWvdsmWLzjvvPElSu3btZLPZNH78+Oi04XBYd9xxh9q3b6/MzExNnz69iX+NpmuxILJ9+3ZNmjRJzz//fKMvoDRjxgx5vd7okJOT01LlAQDiwIEDB/T+++9rwoQJSk1NPeI0/74LY/r06frud7+r9evX69prr9Vrr72mSZMm6bbbbtOGDRv085//XNdcc40+/PDDJtfz61//Wo888ohWrFghh8Oha6+9Nvrcyy+/rOnTp+v+++/XihUrlJWVpT/96U9Nnsc3Pffcc3I6nfrkk080e/bs406fk5OjV155RZJUWlqq3bt3649//GOD90tNTdXSpUv10EMP6Xe/+53mzZt3UjUej6Ol3njlypUqLy/XoEGDouNCoZAWLVqkxx9/XIFAQHa7vcFrpk2bpilTpkQf+/1+wggAmFRdLZWUtP588/OllJTjTrZp0yZZlqU+ffo0GN+xY8doa8OECRP04IMPRp/78Y9/rGuuuSb6+Ec/+pHGjx+vm266SZI0ZcoULVmyRA8//HC09aCx7rvvPo0cOVKSNHXqVF166aWqra1VUlKSHn30UV133XW67rrrJEn33nuvPvjgg2O2ihxP79699dBDD0Ufb9my5ZjT2+12tW/fXpLUuXNnpaenN3i+f//+uvvuu6Pv/fjjj2v+/Pm68MILT7jG42mxIHLBBRdo/fr1DcZdc801ys/P15133vmtECJJLpdLLperpUoCADRVSYk0eHDrz3flSukbP2SbatmyZQqHwxo7dqwCgUCD54YMGdLgcXFxsW644YYG40aMGNGgpaCxvnl0aFZWliSpvLxcubm5Ki4u1i9+8YsG0w8bNuyEWl4OG9zMf5t/P7o1KytL5eXlzTqPf9diQcTtdqtfv34NxqWmpqpDhw7fGg8AaKPy8yOhwMR8G6FXr16y2Wzf6ovRo0cPSVJycvK3XnO0XThHk5AQ6cVgWVZ03NE6uSYmJkbvH94lFA6HmzS/pvj3z9KUWo/km/VLkc/QkvVLLRhEAABxICXlpFomWlqHDh104YUX6vHHH9fNN9/c5JAhSQUFBfrkk080bty46LhPPvlEhYWFkqROnTpJknbv3q2BAwdKUoPOoE2Zz9KlS/Wzn/0sOm7JkiVNfp9jaUytTqdTUqS7RFvQqkHkcO9iAACay5/+9CeNGDFCQ4YM0fTp09W/f38lJCRo+fLlKikpOe7ui1/+8pe6+uqrNXDgQI0ePVpvvfWWXn311ehhv8nJyTrrrLP0wAMPKC8vT+Xl5fqv//qvJtc5adIkjR8/XkOGDNGIESP0/PPP69NPP4223jSHxtTarVs32Ww2zZ07V5dccomSk5OVlpbWbDU0FdeaAQDEtJ49e2r16tUaPXq0pk2bpgEDBmjIkCGaOXOmbr/9dt1zzz3HfP0VV1yhP/7xj3r44YfVt29fPfnkk5ozZ45GjRoVneaZZ55RfX29Bg8erMmTJ+vee+9tcp0/+MEPdNddd+mOO+7Q4MGDtXXrVt14441Nfp/jOV6tXbp00W9/+1tNnTpVGRkZmjhxYrPX0BQ265s7ktoYv98vr9crn88nj8djuhwAiFu1tbUqKytTXl5eo0+5gFPbsb4zTdl+0yICAACMIYgAAABjCCIAAMAYgggAADCGIAIAAIwhiAAAotrwgZRoY5rru0IQAQBET+1dXV1tuBLEisPflX8/LXxTcYp3AIDsdrvS09OjFzhLSUmJXisF+CbLslRdXa3y8nKlp6cf8SK2TUEQAQBIkjIzMyWpxa+2iviQnp4e/c6cDIIIAEBS5EqrWVlZ6ty5c5Ou2IpTT2Ji4km3hBxGEAEANGC325ttIwMcD51VAQCAMQQRAABgDEEEAAAYQxABAADGEEQAAIAxBBEAAGAMQQQAABhDEAEAAMYQRAAAgDEEEQAAYAxBBAAAGEMQAQAAxhBEAACAMQQRAABgDEEEAAAYQxABAADGEEQAAIAxBBEAAGAMQQQAABhDEAEAAMYQRAAAgDEEEQAAYAxBBAAAGEMQAQAAxhBEAACAMQQRAABgDEEEAAAYQxABAADGEEQAAIAxBBEAAGAMQQQAABhDEAEAAMYQRAAAgDEEEQAAYAxBBAAAGEMQAQAAxhBEAACAMQQRAABgDEEEAAAYQxABAADGEEQAAIAxBBEAAGAMQQQAABhDEAEAAMYQRAAAgDEEEQAAYAxBBAAAGEMQAQAAxhBEAACAMQQRAABgTIsGkRkzZuiMM86Q2+1W586ddcUVV6i0tLQlZwkAAGJIiwaRhQsXasKECVqyZInmzZunuro6XXTRRaqqqmrJ2QIAgBhhsyzLaq2Z7du3T507d9bChQt17rnnHnd6v98vr9crn88nj8fTChUCAICT1ZTtd6v2EfH5fJKk9u3bt+ZsAQBAG+VorRmFw2FNnjxZI0aMUL9+/Y44TSAQUCAQiD72+/2tVR4AADCg1VpEJkyYoA0bNujFF1886jQzZsyQ1+uNDjk5Oa1VHgAAMKBV+ohMnDhRb7zxhhYtWqS8vLyjTnekFpGcnBz6iAAAEEOa0kekRXfNWJalm2++Wa+99poWLFhwzBAiSS6XSy6XqyVLAgAAbUiLBpEJEybohRde0BtvvCG32609e/ZIkrxer5KTk1ty1gAAIAa06K4Zm812xPFz5szR+PHjj/t6Dt8FACD2tKldMwAAAEfDtWYAAIAxBBEAAGAMQQQAABhDEAEAAMYQRAAAgDEEEQAAYAxBBAAAGEMQAQAAxhBEAACAMQQRAABgDEEEAAAYQxABAADGEEQAAIAxBBEAAGAMQQQAABhDEAEAAMYQRAAAgDEEEQAAYAxBBAAAGEMQAQAAxhBEAACAMQQRAABgDEEEAAAYQxABAADGEEQAAIAxBBEAAGAMQQQAABhDEAEAAMYQRAAAgDHxEUTq66UtW6T9+01XgpZgWdLOndK2bZH7gEnl5ZH1TThsuhK0hIMHpU2bpLo605WcMmI/iMydK/XqJeXlSZ06SYMHS3PmSMFgs80iGIysd774QtqzR/L7pfpgWKqtlQKByBe2vj6+V0yhUORzBoORz1xb2/KhoKZGevxx6bTTpK5dpW7dFMjvr/C/lrTsfIEj2bJFuvBCKSMjsr7JzpbuuiuyUmhJlhX5nwsEIv9/dXWR/8d4DeWWFVmX1tdHPm9tbeTztvQ8339fOv98qX17qXdvKTNT+uMf43c5tyE2y2q7S9nv98vr9crn88nj8Xx7gnnzpEsukUaPlm65JZJk//pX6d13pZwc6c47peuuk5KSjjmfqipp69avhz2lPtk2rFfq5nVK3bdF6ZXb1VU7lKk9SlOl3KpQmqqO+F4Bm0u19jTVJLoVTExTwOVW0OVWXbJX9ale1ad5FXZ7ZXm8snm9sqV7ldDOK0cHrxI7euXs5JWrs1cp3kSlpEjJyZLDcXLLMVwb1M5iv8rW+rXjM7/2bvTpwBc+OWt88loH5bF8cls+ecIH5Q75lFZ/UCn1PqXWHVRK0KekoE/OUO233tey2VTnTFV9klt1KR4F0jNV2zlHgc45CmbnqbZXP9X16acEr1tOp+R0SomJ37612yM/MnfujAzlmyvU/f2ndM6yh+WpLddbqT/SC4Hvqa5eukMPaUDCelX9c5k6jSw8uQUDNFL9nv0KDB6mUG2dlv3HPapN66hun76tPkv/ooRQnYrPvkFrLrpDVeldZLN9/boj3f/mOHsoqPTdxWq/Y5085Z8r9cvtSv1qh1IO7pKzxqfE2ko5ApVKCH97QxxOsKs+KU11SW7Vu9JUlxy5DaZ4VZfiVV2yV3XJHtWlHrqf8vVQn+KJ3g8lpUaLOl69x/1s4ZASayvkqIqsXxzVfiVW+yJD1cHI8I37jm/er4rctwe/va6RpJArWfXJboWS0xT0dlJtx64KdOyqmoxuqsotVFWPIgXaZ0k22zFrD4Ukny8yHPwyrIyV7+jsBfeq+96l2tjuTL3Z+Xptru+ms8tf1diK2Vo1/jENmnPzEWvC0R13+/0NsRtEqqoiv5T79ZPefrvh1vqzz6T77pNefFFWRoZqfvpzbR14hUodfbVlhyMaOHaWBeUsK1UX36fqpw3qr3Xqr3Xqrq2SpPqERPm8uQp0zpG65qiuQ6ZqnB7VJKSpOiFNNVaSggFLwYClukBYdYGwVF2thOpKOWoqZK+tlDNQoaSAX8l1PqXU+ZQW8skd9slj+ZSsI//DSVK1kuWTVz55VWHzKJRwaIudYI/cOiK3NissRygoe7hOjnBQDisoRzgoZ7hWqeEKpYX9clmBo86nLsGpqsR0VTu8qnSkq9Luld+ergqbV36bVz5bug5aXlVYqaoL2VVXb1NdKEH19VJifY1SQhVKsSrlkV/Z2qWu2qEcbVeOtsuuSAtRmbprvYq0XkVap/5aryJt1GkK6eu/mVMBDdIqfU+v6Fo9I7cqNC/rZ5p/5jQl5vdSly6RH6BJ4Wr1+fEgBZPTVfDlJ7I57CfwzQKa5pPe49Vn01wNS1imrfYekiI/lL3WQU0MP6ZJ1n8rRdV6Xj/RS7YfarGGqVJpX7+BFVautqmvPlVffRpd1xSoWImqlyTtVPah/5wc7VK2DipdlUpThdyqVoos2ZSgsGyylKCwXAooTZXf+HEUufUeWnN45I/ed6tCCTryqr5edvnlkU9e+eVRpdIUkl1hJXzrNlF1cioop4JyKRC9TVbNoflUHnUZ1smhg0rXQaXLJ2+D22/er1FydH6Hh0TVNfiMnVWuHEV+IHbXFqWoRpJ0QO2/ta7ZoH6q+sbfwqawemqz/kNz9f8SnlHf8AatTh6uv3T/jUq7XSRvuk1er5SeLg3+yy26bM/TchWvla3PaSf5LTq1nBpB5N57pXvukUpKIs2khyxdKi1YEMkilas26vLSB3VF3f/JowrVKEl7bZmyHE6l2w7KU3dAdivySyPQIUvhfv3lOmOAEk7vL/XvL/XpE/nZ3gIsSwpWBlWzx6favT4F9/lUt9+n+gM+hb70yToYGeT3KaHCr3CwXuG60NdDfUhWfUiyJSjkcCpsdypsT1TY4YwOgSSP6pI8cnXyqFNPj7LyvercyyN7ujvyX+b1Hre1qDHC4a/32hxuOQ76a2UrKZb9s/VylGyQq3Sdkjatl2v/rshrHIkKuDuqLtkje6BayQd3KyFUr3CnzrL9+Mey3TYl0qp1BMv/+2OdMeUcrbjjZQ158KqTrh84lrI316vb5QO08KonNOqlGxv8yo7y+yO7Ef/8Z6msLPLzOztbSkuL/Gjavz+yi0GKjCsqktV/QGQ907+/rH5F0qF13OE18jfXzMe7f9xpw2FZ/grZKvzR5gCb/9Bthf/r+36fVF0l1YckK/z1bSgU+Ue3O2S5XFKiU5bTJTkjt5YrSWGPV9ah1t5wmkeWxyvL7VHY7VXYky4lJ8uS7Vu1nfTnDIdl31amxJL1cpauV2LJeiWWrldi2UbZDu0uD7XroJC3vWzhkOwHypVQVSnL6ZTtkkuk226Tzj5bR/L2/1Wr39UF6vjdc5X66l+POA2OLP6DSF1dZCP1ve9JTzwhKfKl/O1vI4PHIxUWRob8fKlHdq0KfYvV5ct1cleXyxaoldq1kzp3lgoKpL59I/sF0fIOHJA2bJA+/TSycvb5pJQUKStLGjRIGjKkUfuiVnlHKVH1KvJ93ApF41T2r4LrlLNxvjr7PpcrLfHYE1tW5Pu9cmWkU1lVVSR4HO530LevlJurI6cZNKuaGqm4WFq/Xtq9W/rySykhQerQQSoqkoYPj4a/o9m3T7qv86P6g/2XSthSFumrhkaJ/yDyxhvSFVdIa9ZIAwYoHJYmT5ZmzozskZk6NfJ9Q/xafMdrGvb7K1X68lr1uaq/6XIQp2oPVKmuY6ZWjLxd5y2423Q5MGBgrwot3tZFSf91u/Sb35guJ2Y0JYjE5ub6L3+J/HoeMEB1ddK4cZFW0SeflH71K0LIqWDI3ZfKZ/Nq23+/YroUxLGND7wqtyrV5VfjTJcCQwac7daH7sukV1jXtJTY22QHAtJ770nf/75qaiJ7Z156Sfrf/5VuuMF0cWgtialOfdbjMnVb9arpUhDHbG+9qZWJQ9X7wu6mS4Ehw4dLfz74fWndOmnjRtPlxKXYCyKLFklVVaoceam+8x3pgw+kN9+UfvAD04WhtdX95/d0WmCD9i3eZLoUxKP6euVtnqeSvEvo0nEKGz5ceid8sUKu5MjGBs0u9oLIO+8olN1VIycWae3ayKlELr7YdFEwIf+m81Uvu8qe+dB0KYhD1r8WK63ep8pzvmO6FBhUWCg5Pcna2nWE9CHrmpYQc0Ek+P6Her3yQu3cZdPChdKIEaYrgimde3n0WfJghT9cYLoUxKHK1z/QfnVQxiWDTZcCgxISpGHDpEX28yMt8pz6vdnFVBD5fIVPjs/WaWni2fr448gh+Di1lReOUt6WDzkNM5pd7T8/0ScaoYGDY2o1iRYwfLj0/O7zpcrKyKHZaFYx8x+2apX06wuWKEGWbnvtbPXqZboitAWpl4xSRmi3diygnwiaUX29PMVLtSZ5uHJzTRcD04YPlz6sGKxwcqr00Uemy4k7MRFEPv5YGjVKujjtY4U7dlLG2b1Nl4Q2ovdPhkqSdry6zHAliCvr18sVrNRXhSPoqAoNHSpZCQ7tzRksLV9uupy4ExNB5MorpTPPlH7WZ4kShg/jrISI6nhae21x9FTwX6wc0IwWL1ZQiUo+Z4jpStAGuN2Rk7GudZ4pLeNHT3OLiSBy0UXS23MtOdaviZzIDPiGHVlnqN0mggiaT+3iVdqgfio64+SvxYT4MGyY9N5XZ0aumLp3r+ly4kpMBJFnn5Vc+3dGrk1y+ummy0EbExxwpnr6VysUqDddCuJEcNkardHp/O5B1PDh0ms7z4g8YPdMs4qJIOJwKHJdGUkaONBkKWiD0kcPUYpqVPZOselSEA/q6pTyxQZ9lni6etMdDYcMGyZtVTcF3e2/3h6hWcREEJEkrV4duWLuUS4Nj1NXr+8WSZJ2/mO94UoQF0pL5agPqPq002W3my4GbUXPnlKnTjbtbFcUOd07mk3sBJE1ayKtIXRUxb/x5KZrlyNHtcsJImgGh37tpgwbYLYOtCk2W6RVZG24SFrPuqY5xU4QWb2a/iE4qj2diuTeysoBJ69u+Rp9oTwVnOU1XQramOHDpfn7+svauFGqrTVdTtyIjSBy8KBUVkb/EBxVTc8i5RwkiODkVf0r0lGV1Q3+3bBh0vJAkWzhsPTZZ6bLiRuxEURKSiK3nNMdR+EYVKSc8Dbt3+wzXQpinKtkrdYnDFDfvqYrQVszZIhUaj/0xWD3TLOJjSBSWhq58tBpp5muBG1Ux1GRDqvb391guBLEtP37lVy5X5W5feVymS4GbU1KitRroFt703rQYbUZxU4QycuTkji5EI4s58J81ckh/yf8SsFJKI4cAp50er7hQtBWDR8uraPDarOKnSBSUGC6CrRhzjSntrj6KOFTfqXgxNV/WqKQEpR5DicQwZENGyYtqS5SaB2tr80lNoLIxo0EERxXead+8mynAxlO3FefFOsL9dCAM9kvgyMbNkwqUb7se3dLfr/pcuJCbASRbdsIIjiuQI8CZfs5uypOXGBtiYpVoAGcQgRHkZsrHeh4aNddaanZYuJEbAQRiSCC43INLFSncLkObtpvuhTEqOStxSpvly+323QlaKtsNqnjiD6RB4eP6MRJIYggbnQ4p1CStGMerSI4AdXVauffqrperGtwbIPOTdMOW1eFPiWINIfYCCIZGZKXsxzi2HIv6K162eVfQj8RNF249HMlyJL7DI6YwbENGyYVW/nyLyOINIfYCCJ9+piuADEgJd2prY5eCn9Kiwiabu+CyPemywUEERzboEHSpoQ+ChcTRJoDQQRxZU+HQqVuo0UETXfgkxLtUYaKzm1nuhS0cS6XVJ2bL0/5Jqm+3nQ5Ma/Fg8gTTzyh7t27KykpSUOHDtWyZcua/iacURWNVNWtUFlfEUTQdKFPi/WFq0AdO5quBLEgbUi+EsNBacsW06XEvBYNIi+99JKmTJmiu+++W6tWrdKAAQM0ZswYlZeXN+2NaBFBI9n7FiizfqeC+zm+H03j3lEiXya7ZdA4ORdGvisHPmH3zMlq0SDyhz/8Qddff72uueYaFRYWavbs2UpJSdEzzzzTtDcaPLhlCkTcSR8eOXJm5wf0E0HjWfUhZVdyBmc03umXdlGlUrVzPkHkZLVYEAkGg1q5cqVGjx799cwSEjR69GgtXrz4iK8JBALy+/0NBklSWlpLlYk4kzO6j8Ky6cuP2T2DxtuzdKuSFJB3KC0iaJzsLjaVOfuoZjVB5GS1WBDZv3+/QqGQMjIyGozPyMjQnj17jviaGTNmyOv1RoecnJyWKg9xqlO3FG1NyFPdWoIIGm/be5EWtO4XE0TQeP6sfLm2cnbVk9WmjpqZNm2afD5fdNi+fbvpkhBjbDZpl6dASWXsmkHj+ZaWqEopyjqjq+lSEEPsffPVpaJEtbWmK4ltLRZEOnbsKLvdrr179zYYv3fvXmVmZh7xNS6XSx6Pp8EANJW/a6E67adFBI1nKynWTne+bPY29dsMbVync/PVSfu17p9cVuJktNh/ndPp1ODBgzV//vzouHA4rPnz52vYsGEtNVtAVkGhsgJbZFVWmS4FMSJ9b4kqu9JRFU2Te+jImc3vsHvmZLRo/J8yZYqefvppPffccyouLtaNN96oqqoqXXPNNS05W5zi3EMLlSBL+z9h5YDjO3BAyguUyNGP/iFomsTC3grLpq/+xa7gk+FoyTf/wQ9+oH379uk3v/mN9uzZo9NPP13/+Mc/vtWBFWhO2edHNih7Fxar05hBhqtBW7fhw30aqQMKjCCIoImSkvRVeg8llBbLsiJ91NB0LRpEJGnixImaOHFiS88GiOpW5NF2dVXtSvqJ4Ph2/jNy+GXW+eyaQdPV9S5Q7vJibd0qde9uuprYRM8sxB2HQ9qWWqjETQQRHF/VihKFlKCE03qZLgUxyDO0UAUq1lFOj4VGIIggLn2VWaD2ewgiOL7ETcU64O0RuZIZ0EQpgwuUpy1asZDO8SeKIIK4VH9aobKqN0uBgOlS0IZVVkqdvypRbXf6h+AEFUYuK7FnEZ3jTxRBBHEpeXChHAqpas3npktBG7ZunZSvErkG0D8EJyg/EmLtJZ+pikaRE0IQQVzqPDKyYdn7IbtncHRrl9Sou7aoA0fM4ER5PApmdFUfq1grVpguJjYRRBCXeg3toL3qrMplBBEc3Z5FG5Ugi3OI4KQkFhWov+Mz/etfpiuJTQQRxCW3W9rsLJRKONEQji6w9tD3I58gghNnKyzQACdHzpwoggji1v6MQnl30CKCIwsGJe+2DaryZEnt25suB7GssFBdajZpxb+CsizTxcQeggjiVrhPobIqSqX6etOloA369FOpX3itgoUDTJeCWFdQILsVUrsDn2vTJtPFxB6CCOKWZ2iBnKqTb9Vm06WgDVq9WuqvdUo9q7/pUhDrDh3C21efsnvmBBBEELe6XBhZOWx/n34i+LaSxV+pm7bJeQYtIjhJHTtK2dka3WENHVZPAEEEcavn8Ax9qXbyLaafCL6tcvH6yJ3+tIigGQwapGHJq2kROQEEEcQtR6JN29IKZSshiKChUEhK+Xyt6u1OqU8f0+UgHgwcqF7+VVq/zpLfb7qY2EIQQVyr6FKg9N3smkFDmzZJfYLrVNO9UEpMNF0O4sGgQUr2lytTu7VsmeliYgtBBHHN1q9Q3WuKFagJmy4Fbcjq1dIArVUi/UPQXAYOlCSdk7aafiJNRBBBXOtwbj+lqEYb3+aaM/ja2uVB9dc6JZ010HQpiBe5uVL79vqPrFX0E2kiggjiWt7VZ0iS9r6xxHAlaEv8H61VkgLSWWeZLgXxwmaTBg7UGY5Ih9UwjbCNRhBBXEvKTNcXSYWyLSWIIMKyJPenSyIdVU8/3XQ5iCeDB6v7vmXy+SyVlJguJnYQRBD39vY4S1nbCCKI2L5dKqpeIn+vQZLLZbocxJOzz1bS/p3qYdtCP5EmIIgg7tmGDVOfwDp9ubXCdCloA1avlobrX0o8m90yaGYjRkiSftT1I/qJNAFBBHEv64cjZVdYZc8uNF0K2oDN729WnrYo7T/PN10K4k379lJRkS5O+4gWkSYgiCDu5Z7fS1sTuqv+3XmmS0Eb4PhwnkI2u2yjRpouBfHo3HM14MA/VVJi6csvTRcTGwgiiHu2BJs+63Khsja8b7oUGGZZUvdN87Qz5yzJ4zFdDuLRJZfIXf6FClSsJXRNaxSCCE4JFcPGKLeqRNYmrsR7Ktu8oUaj6uYpMHKM6VIQr84/X1ZKin6U+hb9RBqJIIJTQuY131GlUrXtwRdMlwKDdjz1jjyqUMcJPzBdCuJVUpJsF16oq5yv00+kkQgiOCWcfVGKPnB/VwkvPC8rbJkuB4Ykv/aCilMGqd3Q00yXgnj2k58o/6slqli8QfX1potp+xymCwBaQ0KClHnnOOX819/0/p0f6KLfX3j8F23ZIi1aJJWUSAcPRs6c2LGj1KWL1K+f1Lev5PW2dOmoqZFKS6XPPpM+/zzyt6ipiSz7nj0jZ0ctKor8fY5h37IyDd75hj687L9V0DqV41R1+eUKts/Qz76crQ0bHj/+efP8fmnBAmn9emnPHikYlNq1kzIypIKCyPqmS5fjfsdjlc2yrDb789Dv98vr9crn88lDxzKcLMvS5k5Dte+gUwmffCS3xya7PRJSov/fwaDS3n5J6c8+KtenqyRJdVk5CrfrKFlh2b/aL/u+PbKFQpHnsnMVzO+vYJ8i1eX3VzC/v+ryTpMcR874jVmPHHUay5KtploJFb7I4D8YvW87/Nh/6HFVhWz1dbLV10uHb0P1kVubTVZiouRIjNzaHbISE2U5XbJS3bLS3AqneSK3qW5Zbo9CqR5Zbo/Cbq9Cbq+sNI+sBPs3Szvm/UY9Hw7LsXennCXr5CpZI1fJWiUVr1Hi9s2yHTpfdn3HTNV728tyJclecVCJu7bKFgop0K23vvz+Dfrqqp8rlOI+4ry+uuoGFXz+hpw7ypSenXL8PwRwEup+PV2h+x/QH/5fsa64NU/S1//bh29dxWvU4an7lTb/DSXUBRXypKs+K0eWI1F231ey79+jhNoaSVLIk65gnyIF+/SPrnOCp/WT3F9/37+57jjp+8Fgw3WM7wj3/Qdlq/DLVheUDq9jQvVSfb0qamvV6eN/Nmr7TRDBKcX3f+/Le/UY3aI/aqZuiY53KqBr9Yx+pfuVox16R9/RHF2j93WR/GrY6uFSrfqoVEVaHx36a526aqckKSCndqirdqqLdqqL9ipDVUpVpdJUpVQF5FKCwtHBoXqlqkoe+eVWRYPBK5/SdVBe+eSVT4k6cjtvSAnyy6ODSpdPXlXIraCcqpejwRCSXTZZSlSdElUnh+qj910KROfrkV9pqlSCjr56qFCafPJG53mkwS+PwkqQTZZsh97LpUD0M6XroDK1R921Rd20VS4FJUkH5dVaDdAana71KtKn6qtiFcin9AY1pKhKZ+tjjdXz+qFeVIXc+m/dqpm6ucHfbYz+oX/oO5p36aO6cO6kxn9hgBNVWakvM/K1orpQl+pt1Ssx+tRQLdGvdZ8u01xtVg/N1M16Q5dri7pL+joR2BRWN21VP21osK7po1I5FPkxtEtZ0fXNbmXJL090XVOjZNlkNVjfpKj6W+uZw+uaw0O6DipZtUf/aEqN/t/75VGtkqLrmDolql4O+WXpGr1OEAGOpOam25T05KPa9f1J+qr/SKVsXqfst5+Wa98O7T3/R9r641+pOq9vg9cc77/EsiSH/0ulla1X2pYNcpVvU9K+HXLt36lE3z7Za6siQ02lEuqDshLssmwJUkKCLFuCQslpCiW7VZ/ibnBbl5au+lSv6lO8qj90v+4I90PJaQ1+0jSm3uOxQmE5gtVy1FTIUeVTYo1fjipf5H515NZx+LbKp8Sqr+8fHm+v8stmHbr6l+1QHEl0Ruo//BnadVZtZncFsvJUm9ldVT2KFMzMjX6eI/1qO9I4597t6vL8Q+r85tMKu5K177LrVDHgbCVt36guz9yjmiHnKO3DuZEmMKAV1Lzxvlzf/w9VDDhHe757oxz+L9Xp3b/Is/4TVXcr0Paf/kr7LvihLHukBfV4LYeH7ycEa5W8pVhpX6xT8p4yOQ+ta1xf7pa9pvLrIVjTYD0jm00hV4rqk79ezxy+X5fijaxrUr0KpqarPsWrutT06Pjo/RSPLLvjuLVWVfk1dmzjtt8EEZx6QiHpwQelGTOkykopNVX63vekqVMj+2MR23btkn7/e+lvf5P2749cT+ZHP5JmzpTS0kxXh1PNe+9JkyZF+jklJEjnnBN5fPnlcR2Km7L9Jojg1BUKSeXlUufOkt1+/OkRW8Jhad++SKfWpCTT1eBUZlmRUJycfMqE4aZsvzlqBqcuu13KyjJdBVpKQkLkqAPANJtN6tTJdBVtVvy2CwEAgDaPIAIAAIwhiAAAAGMIIgAAwBiCCAAAMIYgAgAAjCGIAAAAYwgiAADAGIIIAAAwhiACAACMIYgAAABjCCIAAMAYgggAADCGIAIAAIwhiAAAAGMIIgAAwBiCCAAAMIYgAgAAjCGIAAAAYwgiAADAGIIIAAAwhiACAACMIYgAAABjCCIAAMAYgggAADCGIAIAAIwhiAAAAGMIIgAAwBiCCAAAMIYgAgAAjGmRILJlyxZdd911ysvLU3Jysnr27Km7775bwWCwJWYHAABilKMl3rSkpEThcFhPPvmkevXqpQ0bNuj6669XVVWVHn744ZaYJQAAiEE2y7Ks1pjR73//e82aNUtffPFFo1/j9/vl9Xrl8/nk8XhasDoAANBcmrL9bpEWkSPx+Xxq3779MacJBAIKBALRx36/v6XLAgAABrVKZ9VNmzZp5syZ+vnPf37M6WbMmCGv1xsdcnJyWqM8AABgSJOCyNSpU2Wz2Y45lJSUNHjNzp07dfHFF+uqq67S9ddff8z3nzZtmnw+X3TYvn170z8RAACIGU3qI7Jv3z4dOHDgmNP06NFDTqdTkrRr1y6NGjVKZ511lp599lklJDStAYY+IgAAxJ4W6yPSqVMnderUqVHT7ty5U+edd54GDx6sOXPmNDmEAACA+NcinVV37typUaNGqVu3bnr44Ye1b9++6HOZmZktMUsAABCDWiSIzJs3T5s2bdKmTZvUtWvXBs+10tHCAAAgBrTI/pLx48fLsqwjDgAAAIfRcQMAABhDEAEAAMYQRAAAgDEEEQAAYAxBBAAAGEMQAQAAxhBEAACAMQQRAABgDEEEAAAYQxABAADGEEQAAIAxBBEAAGAMQQQAABhDEAEAAMYQRAAAgDEEEQAAYAxBBAAAGEMQAQAAxhBEAACAMQQRAABgDEEEAAAYQxABAADGEEQAAIAxBBEAAGAMQQQAABhDEAEAAMYQRAAAgDEEEQAAYAxBBAAAGEMQAQAAxhBEAACAMQQRAABgDEEEAAAYQxABAADGEEQAAIAxBBEAAGAMQQQAABhDEAEAAMYQRAAAgDEEEQAAYAxBBAAAGEMQAQAAxhBEAACAMQQRAABgDEEEAAAYQxABAADGEEQAAIAxBBEAAGAMQQQAABhDEAEAAMYQRAAAgDEEEQAAYAxBBAAAGEMQAQAAxhBEAACAMQQRAABgDEEEAAAYQxABAADGEEQAAIAxBBEAAGAMQQQAABhDEAEAAMYQRAAAgDEtHkQCgYBOP/102Ww2rVmzpqVnBwAAYkiLB5E77rhD2dnZLT0bAAAQg1o0iLz77rt6//339fDDD7fkbAAAQIxytNQb7927V9dff71ef/11paSkNOo1gUBAgUAg+tjv97dUeQAAoA1okRYRy7I0fvx4/eIXv9CQIUMa/boZM2bI6/VGh5ycnJYoDwAAtBFNCiJTp06VzWY75lBSUqKZM2eqoqJC06ZNa1Ix06ZNk8/niw7bt29v0usBAEBssVmWZTV24n379unAgQPHnKZHjx66+uqr9dZbb8lms0XHh0Ih2e12jR07Vs8991yj5uf3++X1euXz+eTxeBpbJgAAMKgp2+8mBZHG2rZtW4P+Hbt27dKYMWP097//XUOHDlXXrl0b9T4EEQAAYk9Ttt8t0lk1Nze3weO0tDRJUs+ePRsdQgAAQPzjzKoAAMCYFjt895u6d++uFtgDBAAAYhwtIgAAwBiCCAAAMIYgAgAAjCGIAAAAYwgiAADAGIIIAAAwhiACAACMIYgAAABjCCIAAMAYgggAADCGIAIAAIwhiAAAAGMIIgAAwBiCCAAAMIYgAgAAjCGIAAAAYwgiAADAGIIIAAAwhiACAACMIYgAAABjCCIAAMAYgggAADCGIAIAAIwhiAAAAGMIIgAAwBiCCAAAMIYgAgAAjCGIAAAAYwgiAADAGIfpAo7FsixJkt/vN1wJAABorMPb7cPb8WNp00HkwIEDkqScnBzDlQAAgKaqqKiQ1+s95jRtOoi0b99ekrRt27bjfhCcOL/fr5ycHG3fvl0ej8d0OXGL5dx6WNatg+XcOmJxOVuWpYqKCmVnZx932jYdRBISIl1YvF5vzCz8WObxeFjOrYDl3HpY1q2D5dw6Ym05N7YBgc6qAADAGIIIAAAwpk0HEZfLpbvvvlsul8t0KXGN5dw6WM6th2XdOljOrSPel7PNasyxNQAAAC2gTbeIAACA+EYQAQAAxhBEAACAMQQRAABgTJsOIk888YS6d++upKQkDR06VMuWLTNdUlyZMWOGzjjjDLndbnXu3FlXXHGFSktLTZcV9x544AHZbDZNnjzZdClxZ+fOnfrJT36iDh06KDk5WUVFRVqxYoXpsuJKKBTSXXfdpby8PCUnJ6tnz5665557GnVNERzbokWLdNlllyk7O1s2m02vv/56g+cty9JvfvMbZWVlKTk5WaNHj9bnn39upthm1GaDyEsvvaQpU6bo7rvv1qpVqzRgwACNGTNG5eXlpkuLGwsXLtSECRO0ZMkSzZs3T3V1dbroootUVVVlurS4tXz5cj355JPq37+/6VLizldffaURI0YoMTFR7777rj777DM98sgjateunenS4sqDDz6oWbNm6fHHH1dxcbEefPBBPfTQQ5o5c6bp0mJeVVWVBgwYoCeeeOKIzz/00EN67LHHNHv2bC1dulSpqakaM2aMamtrW7nSZma1UWeeeaY1YcKE6ONQKGRlZ2dbM2bMMFhVfCsvL7ckWQsXLjRdSlyqqKiwevfubc2bN88aOXKkNWnSJNMlxZU777zTOvvss02XEfcuvfRS69prr20w7sorr7TGjh1rqKL4JMl67bXXoo/D4bCVmZlp/f73v4+OO3jwoOVyuaz//d//NVBh82mTLSLBYFArV67U6NGjo+MSEhI0evRoLV682GBl8c3n80n6+mKDaF4TJkzQpZde2uB7jebz5ptvasiQIbrqqqvUuXNnDRw4UE8//bTpsuLO8OHDNX/+fG3cuFGStHbtWn388cf6zne+Y7iy+FZWVqY9e/Y0WH94vV4NHTo05reLbfKid/v371coFFJGRkaD8RkZGSopKTFUVXwLh8OaPHmyRowYoX79+pkuJ+68+OKLWrVqlZYvX266lLj1xRdfaNasWZoyZYp+9atfafny5brlllvkdDo1btw40+XFjalTp8rv9ys/P192u12hUEj33Xefxo4da7q0uLZnzx5JOuJ28fBzsapNBhG0vgkTJmjDhg36+OOPTZcSd7Zv365JkyZp3rx5SkpKMl1O3AqHwxoyZIjuv/9+SdLAgQO1YcMGzZ49myDSjF5++WU9//zzeuGFF9S3b1+tWbNGkydPVnZ2NssZJ6RN7prp2LGj7Ha79u7d22D83r17lZmZaaiq+DVx4kTNnTtXH374obp27Wq6nLizcuVKlZeXa9CgQXI4HHI4HFq4cKEee+wxORwOhUIh0yXGhaysLBUWFjYYV1BQoG3bthmqKD798pe/1NSpU/XDH/5QRUVF+ulPf6pbb71VM2bMMF1aXDu87YvH7WKbDCJOp1ODBw/W/Pnzo+PC4bDmz5+vYcOGGawsvliWpYkTJ+q1117TP//5T+Xl5ZkuKS5dcMEFWr9+vdasWRMdhgwZorFjx2rNmjWy2+2mS4wLI0aM+Nbh5xs3blS3bt0MVRSfqqurlZDQcNNht9sVDocNVXRqyMvLU2ZmZoPtot/v19KlS2N+u9hmd81MmTJF48aN05AhQ3TmmWfq0UcfVVVVla655hrTpcWNCRMm6IUXXtAbb7wht9sd3c/o9XqVnJxsuLr44Xa7v9XvJjU1VR06dKA/TjO69dZbNXz4cN1///26+uqrtWzZMj311FN66qmnTJcWVy677DLdd999ys3NVd++fbV69Wr94Q9/0LXXXmu6tJhXWVmpTZs2RR+XlZVpzZo1at++vXJzczV58mTde++96t27t/Ly8nTXXXcpOztbV1xxhbmim4Ppw3aOZebMmVZubq7ldDqtM88801qyZInpkuKKpCMOc+bMMV1a3OPw3Zbx1ltvWf369bNcLpeVn59vPfXUU6ZLijt+v9+aNGmSlZubayUlJVk9evSwfv3rX1uBQMB0aTHvww8/POI6edy4cZZlRQ7hveuuu6yMjAzL5XJZF1xwgVVaWmq26GZgsyxOhwcAAMxok31EAADAqYEgAgAAjCGIAAAAYwgiAADAGIIIAAAwhiACAACMIYgAAABjCCIAAMAYgggAADCGIAIAAIwhiAAAAGMIIgAAwJj/D/MAPhyHsymXAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction saved as 'Lab4_basic_regression.jpg'\n",
      "Animation saved as 'Lab4_basic_regression.gif'\n"
     ]
    }
   ],
   "source": [
    "save_final_result(model, x_train, y_train)\n",
    "animate_training(history, x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JX-kDur7xKOW"
   },
   "source": [
    "## Part 3: Binary classification (10%)\n",
    "\n",
    "You will train a model to perform binary classification. Your task is to predict whether an optical coherence tomography (OCT) image shows Choroidal Neovascularization (CNV) or is normal.\n",
    "\n",
    "- Data: OCT scan image of retina\n",
    "- Classes:\n",
    "  - CNV: label = 1\n",
    "  - Normal: label = 0\n",
    "\n",
    "- Data Description:\n",
    "  - Input: Grayscale images (28x28 pixels)\n",
    "  - Training set size: 20000 images\n",
    "  - Testing set size: 5000 images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAvUwG1uSLg_"
   },
   "source": [
    "> ### Step 1: Read data & split data\n",
    "Load *basic_data.npz* and prepare it for training by splitting into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Hp8M93z7v_lO"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: x=(20000, 784), y=(20000, 1)\n",
      "Test: x=(5000, 784)\n",
      "\n",
      "After splitting:\n",
      "x_train: (16000, 784) | y_train: (16000, 1)\n",
      "x_val: (4000, 784) | y_val: (4000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data = np.load('basic_data.npz')\n",
    "X_train = data[\"x_train\"]\n",
    "Y_train = data[\"y_train\"]\n",
    "X_test = data[\"x_test\"]\n",
    "\n",
    "# Display sample images with labels\n",
    "class_names_binary = {0: 'Normal', 1: 'CNV'}\n",
    "plt.figure(figsize=(5, 5))\n",
    "for i in range(9):\n",
    "    plt.subplot(330 + 1 + i)\n",
    "    plt.imshow(X_train[i].reshape(28, 28), cmap='gray', vmin=0, vmax=255)\n",
    "    plt.title(f'Label: {int(Y_train[i])} ({class_names_binary[int(Y_train[i])]})', fontsize=8)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Data preprocessing\n",
    "### START CODE HERE ###\n",
    "\n",
    "# Normalize X data to [0,1] range\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "# Reshape Y_train to 2D array\n",
    "Y_train = Y_train.reshape(-1, 1)\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Plot data distribution\n",
    "Y_train_1 = np.sum(Y_train == 1)\n",
    "Y_train_0 = np.sum(Y_train == 0)\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.bar([0, 1], [Y_train_0, Y_train_1])\n",
    "plt.title('Data distribution')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "print('Train: x=%s, y=%s' % (X_train.shape, Y_train.shape))\n",
    "print('Test: x=%s' % (X_test.shape, ))\n",
    "\n",
    "# Train-validation split\n",
    "### START CODE HERE ###\n",
    "# Choose the ratio for splitting\n",
    "split_ratio = 0.8\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "permutation = list(np.random.permutation(X_train.shape[0]))\n",
    "shuffled_X = X_train[permutation]\n",
    "shuffled_Y = Y_train[permutation]\n",
    "x_train = shuffled_X[:int(X_train.shape[0] * split_ratio)]\n",
    "y_train = shuffled_Y[:int(Y_train.shape[0] * split_ratio)]\n",
    "x_val = shuffled_X[int(X_train.shape[0] * split_ratio):]\n",
    "y_val = shuffled_Y[int(Y_train.shape[0] * split_ratio):]\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(\"\\nAfter splitting:\")\n",
    "print(\"x_train:\", x_train.shape, \"| y_train:\", y_train.shape)\n",
    "print(\"x_val:\", x_val.shape, \"| y_val:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r01QzzHxeMbR"
   },
   "source": [
    "> ### Step 2: Training and Evaluation\n",
    "Train your model on the prepared OCT image data and evaluate its performance in distinguishing between CNV and normal retinal conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "fI7JY5ESjhZ2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 0.3276975155126745\n",
      "Loss after iteration 16: 0.2842715973117927\n",
      "Loss after iteration 32: 0.23734881539834898\n",
      "Loss after iteration 48: 0.21582315732447174\n",
      "Loss after iteration 64: 0.21392800211451582\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "loss_function = \"cross_entropy\"\n",
    "layers_dims = [784, 1568, 784, 196, 1]\n",
    "activation_fn = ['relu', 'relu', 'relu', 'sigmoid']\n",
    "learning_rate = 0.01\n",
    "num_iterations = 80\n",
    "print_loss = True\n",
    "print_freq = 16\n",
    "decrease_freq = 32\n",
    "decrease_proportion = 0.7\n",
    "# You might need to use mini_batch to reduce training time in this part\n",
    "batch_size = 256\n",
    "\n",
    "model = Model(layers_dims, activation_fn, loss_function)\n",
    "model, losses, history = train_model(model, x_train, y_train, learning_rate, num_iterations, batch_size, print_loss, print_freq, decrease_freq, decrease_proportion)\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Plot the loss\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Training Loss (Initial LR: {learning_rate})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "U8q0a20XcPtk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training------\n",
      "Accuracy: 84.65%\n",
      "f1 score for each class: [0.85002443 0.84280594]\n",
      "f1_macro score: 0.85\n",
      "validation------\n",
      "Accuracy: 81.85%\n",
      "f1 score for each class: [0.82056352 0.81638847]\n",
      "f1_macro score: 0.82\n"
     ]
    }
   ],
   "source": [
    "print('training------')\n",
    "pred_train = predict(x_train, y_train, model)\n",
    "print('validation------')\n",
    "pred_val = predict(x_val, y_val, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqtnepD-6I20"
   },
   "source": [
    "> ### Step 3: Save prediction\n",
    "Save your model's predictions to: *Lab4_basic.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "mERo3g41zsyX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction data saved as 'Lab4_basic.csv'\n"
     ]
    }
   ],
   "source": [
    "pred_test = predict(X_test, None, model)\n",
    "df = pd.DataFrame({\n",
    "    'ID': range(len(pred_test)),\n",
    "    'Label': pred_test.flatten()\n",
    "})\n",
    "\n",
    "df.to_csv('Lab4_basic.csv', index=False)\n",
    "print(\"Prediction data saved as 'Lab4_basic.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMCpPFMVdj36"
   },
   "source": [
    "# **Advanced Part (30%)**\n",
    "\n",
    "You will train a model to perform multi-class classification on medical imaging data. Your task is to classify optical coherence tomography (OCT) images of retinal conditions into four different categories.\n",
    "\n",
    "- Data: OCT scan images of retina\n",
    "- Classes:\n",
    "  - CNV (Choroidal Neovascularization): label = 0\n",
    "  - DME (Diabetic Macular Edema): label = 1\n",
    "  - Drusen: label = 2\n",
    "  - Normal: label = 3\n",
    "\n",
    "- Data Description:\n",
    "  - Input: Grayscale images (28x28 pixels)\n",
    "  - Training set size: 37754 images\n",
    "  - Testing set size: 6997 images\n",
    "\n",
    "**Notes:** You can implement other functions to improve your rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_GQ3uO128OC"
   },
   "source": [
    "## Step 1: Read data & split data\n",
    "\n",
    "Load *advanced_data.npz* and prepare it for training by splitting into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bVSfqnXqXGdC"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = np.load('advanced_data.npz')\n",
    "X_train = data[\"x_train\"]\n",
    "Y_train = data[\"y_train\"]\n",
    "X_test = data[\"x_test\"]\n",
    "\n",
    "print(f'Initial shapes:')\n",
    "print(f'Train: X={X_train.shape}, Y={Y_train.shape}')\n",
    "print(f'Test: X={X_test.shape}')\n",
    "\n",
    "# Display sample images with labels\n",
    "class_names = {0: 'CNV', 1: 'DME', 2: 'Drusen', 3: 'Normal'}\n",
    "plt.figure(figsize=(5, 5))\n",
    "for i in range(9):\n",
    "    plt.subplot(330 + 1 + i)\n",
    "    plt.imshow(X_train[i].reshape(28, 28), cmap='gray', vmin=0, vmax=255)\n",
    "    plt.title(f'Label: {int(Y_train[i])} ({class_names[int(Y_train[i])]})', fontsize=8)\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Data preprocessing\n",
    "### START CODE HERE ###\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = 4  # OCT has 4 classes\n",
    "Y_train = np.eye(num_classes)[Y_train]\n",
    "\n",
    "# Normalize X data to [0,1] range\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(\"\\nAfter preprocessing:\")\n",
    "print(\"shape of X_train:\", X_train.shape)\n",
    "print(\"shape of Y_train:\", Y_train.shape)\n",
    "print(\"shape of X_test:\", X_test.shape)\n",
    "\n",
    "# Plot class distribution before splitting\n",
    "orig_labels = np.argmax(Y_train, axis=1)\n",
    "unique, counts = np.unique(orig_labels, return_counts=True)\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.bar(unique, counts)\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Train-validation split\n",
    "### START CODE HERE ###\n",
    "# Choose the ratio for splitting\n",
    "split_ratio = 0.8\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "permutation = list(np.random.permutation(X_train.shape[0]))\n",
    "shuffled_X = X_train[permutation]\n",
    "shuffled_Y = Y_train[permutation]\n",
    "x_train = shuffled_X[:int(X_train.shape[0] * split_ratio)]\n",
    "y_train = shuffled_Y[:int(Y_train.shape[0] * split_ratio)]\n",
    "x_val = shuffled_X[int(X_train.shape[0] * split_ratio):]\n",
    "y_val = shuffled_Y[int(Y_train.shape[0] * split_ratio):]\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(\"\\nAfter splitting:\")\n",
    "print(\"x_train:\", x_train.shape, \"| y_train:\", y_train.shape)\n",
    "print(\"x_val:\", x_val.shape, \"| y_val:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngmUDGN13ADi"
   },
   "source": [
    "## Step 2: Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIi1A-1dFY0u"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "loss_function = \"cross_entropy\"\n",
    "layers_dims = [784, 1568, 784, 196, 1]\n",
    "activation_fn = ['relu', 'relu', 'relu', 'softmax']\n",
    "learning_rate = 0.01\n",
    "num_iterations = 80\n",
    "print_loss = True\n",
    "print_freq = 50\n",
    "decrease_freq = 32\n",
    "decrease_proportion = 0.7\n",
    "batch_size = 256\n",
    "\n",
    "model = Model(layers_dims, activation_fn, loss_function)\n",
    "model, losses, history = train_model(model, x_train, y_train, learning_rate, num_iterations, batch_size, print_loss, print_freq, decrease_freq, decrease_proportion)\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Plot the loss\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Training Loss (Initial LR: {learning_rate})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehjcfSU2XD3-"
   },
   "outputs": [],
   "source": [
    "print('training------')\n",
    "pred_train = predict(x_train, y_train, model)\n",
    "print('validation------')\n",
    "pred_val = predict(x_val, y_val, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXGnS3HQeNUc"
   },
   "source": [
    "## Step 3: Save prediction\n",
    "Save your model's predictions to: *Lab4_advanced.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHFDuq2BQ2qI"
   },
   "outputs": [],
   "source": [
    "pred_test = predict(X_test, None, model)\n",
    "df = pd.DataFrame({\n",
    "    'ID': range(len(pred_test)),\n",
    "    'Label': pred_test.flatten()\n",
    "})\n",
    "\n",
    "df.to_csv('Lab4_advanced.csv', index=False)\n",
    "print(\"Prediction data saved as 'Lab4_advanced.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J91ff4Vk1oB_"
   },
   "source": [
    "# Save outputs\n",
    "Save the outputs of your testing codes to: *Lab4_output.npy*\n",
    "\n",
    "We will test your *Lab4_output.npy* to verify the correctness of your neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CpxmIFiW1tg9"
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "assert list(outputs.keys()) == [\n",
    "    'dense_forward',\n",
    "    'dense_backward',\n",
    "    'dense_update_parameters',\n",
    "    'sigmoid',\n",
    "    'relu',\n",
    "    'softmax',\n",
    "    'linear',\n",
    "    'sigmoid_backward',\n",
    "    'relu_backward',\n",
    "    'softmax_backward',\n",
    "    'linear_backward',\n",
    "    'model_forward_sigmoid',\n",
    "    'model_forward_relu',\n",
    "    'model_forward_softmax',\n",
    "    'model_backward_sigmoid',\n",
    "    'model_backward_relu',\n",
    "    'model_update_parameters',\n",
    "    'compute_BCE_loss',\n",
    "    'compute_CCE_loss'\n",
    "], \"You're missing something, please restart the kernel and run the code from beginning to the end. If the same error occurs, maybe you deleted some outputs, check the template to find the missing parts!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uDqCzhsp1yTb"
   },
   "outputs": [],
   "source": [
    "np.save(\"Lab4_output.npy\", outputs)\n",
    "\n",
    "# sanity check for saved outputs\n",
    "submit = np.load(\"Lab4_output.npy\", allow_pickle=True).item()\n",
    "for key, value in submit.items():\n",
    "    print(f\"{key}: {type(value)}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
