{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "eb6ccSWDWrTd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import random\n",
        "from numpy import sqrt\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "input_data = pd.read_csv('lab2_basic_input.csv')\n",
        "input_data\n",
        "max_depth = 2\n",
        "depth = 0\n",
        "min_samples_split = 2\n",
        "n_features = input_data.shape[1] - 1\n",
        "def entropy(data):\n",
        "  \"\"\"\n",
        "  This function measures the amount of uncertainty in a probability distribution\n",
        "  args:\n",
        "  * data(type: DataFrame): the data you're calculating for the entropy\n",
        "  return:\n",
        "  * entropy_value(type: float): the data's entropy\n",
        "  \"\"\"\n",
        "  p = 0 # to count the number of cases that survived\n",
        "  n = 0 # to count the number of cases that passed away\n",
        "\n",
        "  ### START CODE HERE ###\n",
        "  # Hint 1: what is the equation for calculating entropy?\n",
        "  # Hint 2: consider the case when p == 0 or n == 0, what should entropy be?\n",
        "  resultList = list(data.loc[:, 'hospital_death'])\n",
        "  p = sum(resultList)\n",
        "  length = len(resultList)\n",
        "  n = length - p\n",
        "  if (n == 0 or p == 0):\n",
        "    entropy_value = 0\n",
        "  else:\n",
        "    entropy_value = (-p / length) * np.log2(p / length) + (-n / length) * np.log2(n / length)\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  return entropy_value\n",
        "\n",
        "# [Note] You have to save the value of \"ans_entropy\" into the output file\n",
        "# Please round your answer to 4 decimal place\n",
        "ans_entropy = entropy(input_data).round(4)\n",
        "print(\"ans_entropy = \", ans_entropy)\n",
        "def information_gain(data, mask):\n",
        "  \"\"\"\n",
        "  This function will calculate the information gain\n",
        "  args:\n",
        "  * data(type: DataFrame): the data you're calculating for the information gain\n",
        "  * mask(type: Series): partition information(left/right) of current input data,\n",
        "    - boolean 1(True) represents split to left subtree\n",
        "    - boolean 0(False) represents split to right subtree\n",
        "  return:\n",
        "  * ig(type: float): the information gain you can obtain by classifying the data with this given mask\n",
        "  \"\"\"\n",
        "  ### START CODE HERE ###\n",
        "  # Hint: you should use mask to split the data into two, then recall what is the equation for calculating information gain\n",
        "  left = mask[mask == True].index\n",
        "  right = mask[mask == False].index\n",
        "  \n",
        "  left_len = len(left)\n",
        "  right_len = len(right)\n",
        "  total_len = left_len + right_len\n",
        "  \n",
        "  left_subtree = data.loc[left]\n",
        "  right_subtree = data.loc[right]\n",
        "  \n",
        "  left_entropy = entropy(left_subtree)\n",
        "  right_entropy = entropy(right_subtree)\n",
        "  \n",
        "  before_entropy = entropy(data)\n",
        "  after_entropy = left_len / total_len * left_entropy + right_len / total_len * right_entropy\n",
        "\n",
        "  ig = before_entropy - after_entropy\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  return ig\n",
        "\n",
        "# [Note] You have to save the value of \"ans_informationGain\" into your output file\n",
        "# Here, let's assume that we split the input_data with 2/3 of the data in the left subtree and 1/3 in the right subtree\n",
        "# Please round your answer to 4 decimal place\n",
        "temp1 = np.zeros((int(input_data.shape[0]/3), 1), dtype=bool)\n",
        "temp2 = np.ones(((input_data.shape[0]-int(input_data.shape[0]/3), 1)), dtype=bool)\n",
        "temp_mask = np.concatenate((temp1, temp2))\n",
        "df_mask = pd.DataFrame(temp_mask, columns=['mask'])\n",
        "ans_informationGain = information_gain(input_data, df_mask['mask']).round(4)\n",
        "print(\"ans_informationGain = \", ans_informationGain)\n",
        "def find_best_split(data, impl_part):\n",
        "  \"\"\"\n",
        "  This function will find the best split combination of data\n",
        "  args:\n",
        "  * data(type: DataFrame): the input data\n",
        "  * impl_part(type: string): 'basic' or 'advanced' to specify which implementation to use\n",
        "  return\n",
        "  * best_ig(type: float): the best information gain you obtain\n",
        "  * best_threshold(type: float): the value that splits data into 2 branches\n",
        "  * best_feature(type: string): the feature that splits data into 2 branches\n",
        "  \"\"\"\n",
        "  best_ig = -1e9\n",
        "  best_threshold = 0\n",
        "  best_feature = ''\n",
        "\n",
        "  if(impl_part == 'basic'):\n",
        "    # Implement this part of the function using the method we provided\n",
        "    ### START CODE HERE ###\n",
        "    features = list(data.columns)\n",
        "    for feature in features:\n",
        "      if feature == 'hospital_death':\n",
        "        break\n",
        "      data_sorted = data.sort_values(by = feature)\n",
        "      mask = np.zeros((int(data.shape[0]), 1), dtype=bool)\n",
        "      for i in range(data.shape[0] - 1):\n",
        "        # mask[data_sorted.index[i]] = True\n",
        "        mask[i] = True\n",
        "        if (data.loc[data_sorted.index[i], feature] == data.loc[data_sorted.index[i + 1], feature]):\n",
        "          continue\n",
        "        df_mask = pd.DataFrame(mask, columns = ['mask'], index = data_sorted.index)\n",
        "        ig = information_gain(data, df_mask['mask'])\n",
        "        # print(df_mask)\n",
        "        if ig > best_ig:\n",
        "          best_ig = ig\n",
        "          best_threshold = (data.loc[data_sorted.index[i], feature] + data.loc[data_sorted.index[i + 1], feature]) / 2\n",
        "          best_feature = feature\n",
        "          # print(best_ig, best_threshold, best_feature)\n",
        "    ### END CODE HERE ###\n",
        "  else:\n",
        "    # You can implement another method here for the advanced part\n",
        "    ### START CODE HERE ###\n",
        "    advance = False\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "\n",
        "  return float(best_ig), float(best_threshold), best_feature\n",
        "\n",
        "\n",
        "# [Note] You have to save the value of \"ans_ig\", \"ans_value\", and \"ans_name\" into the output file\n",
        "# Here, let's try to find the best split for the input_data\n",
        "# Please round your answer to 4 decimal place\n",
        "ans_ig, ans_value, ans_name = find_best_split(input_data, 'basic')\n",
        "print(\"ans_ig = \", ans_ig)\n",
        "print(\"ans_value = \", ans_value)\n",
        "print(\"ans_name = \", ans_name)\n",
        "def make_partition(data, feature, threshold):\n",
        "  \"\"\"\n",
        "  This function will split the data into 2 branches\n",
        "  args:\n",
        "  * data(type: DataFrame): the input data\n",
        "  * feature(type: string): the attribute(column name)\n",
        "  * threshold(type: float): the threshold for splitting the data\n",
        "  return:\n",
        "  * left(type: DataFrame): the divided data that matches(less than or equal to) the assigned feature's threshold\n",
        "  * right(type: DataFrame): the divided data that doesn't match the assigned feature's threshold\n",
        "  \"\"\"\n",
        "  ### START CODE HERE ###\n",
        "  left = data[data[feature] <= threshold]\n",
        "  right = data[data[feature] > threshold]\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  return left, right\n",
        "\n",
        "\n",
        "# [Note] You have to save the value of \"ans_left\" into the output file\n",
        "# Here, let's assume the best split is when we choose bmi as the feature and threshold as 21.0\n",
        "left, right = make_partition(input_data, 'bmi', 21.0)\n",
        "ans_left = left.shape[0]\n",
        "print(\"ans_left = \", ans_left)\n",
        "def build_tree(data, max_depth, min_samples_split, depth):\n",
        "  \"\"\"\n",
        "  This function will build the decision tree\n",
        "  args:\n",
        "  * data(type: DataFrame): the data you want to apply to the decision tree\n",
        "  * max_depth: the maximum depth of a decision tree\n",
        "  * min_samples_split: the minimum number of instances required to do partition\n",
        "  * depth: the height of the current decision tree\n",
        "  return:\n",
        "  * subtree: the decision tree structure including root, branch, and leaf (with the attributes and thresholds)\n",
        "  \"\"\"\n",
        "  ### START CODE HERE ###\n",
        "  # check the condition of current depth and the remaining number of samples\n",
        "  if depth < max_depth and data.shape[0] > min_samples_split:\n",
        "    # call find_best_split() to find the best combination\n",
        "    ig, threshold, feature = find_best_split(data, 'basic')\n",
        "    # check the value of information gain is greater than 0 or not\n",
        "    if ig > 0 :\n",
        "      # update the depth\n",
        "      depth += 1\n",
        "      # call make_partition() to split the data into two parts\n",
        "      left, right = make_partition(data, feature, threshold)\n",
        "      # If there is no data split to the left tree OR no data split to the right tree\n",
        "      if (left.empty or right.empty):\n",
        "        # return the label of the majority\n",
        "        # vote for the final result\n",
        "        label = int(data['hospital_death'].mode().iloc[0])\n",
        "        return label\n",
        "      else:\n",
        "        question = \"{} {} {}\".format(feature, \"<=\", threshold)\n",
        "        subtree = {question: []}\n",
        "\n",
        "        # call function build_tree() to recursively build the left subtree and right subtree\n",
        "        left_subtree = build_tree(left, max_depth, min_samples_split, depth)\n",
        "        right_subtree = build_tree(right, max_depth, min_samples_split, depth)\n",
        "        \n",
        "        if left_subtree == right_subtree:\n",
        "          subtree = left_subtree\n",
        "        else:\n",
        "          subtree[question].append(left_subtree)\n",
        "          subtree[question].append(right_subtree)\n",
        "    else:\n",
        "      # return the label of the majority\n",
        "      # vote for the final result\n",
        "      label = int(data['hospital_death'].mode().iloc[0])\n",
        "      return label\n",
        "  else:\n",
        "    # return the label of the majority\n",
        "    # vote for the final result\n",
        "    label = int(data['hospital_death'].mode().iloc[0])\n",
        "    return label\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  return subtree\n",
        "# Here, let's build a decision tree using the input_data\n",
        "\n",
        "decisionTree = build_tree(input_data, max_depth, min_samples_split, depth)\n",
        "decisionTree\n",
        "# [Note] You have to save the features in the \"decisionTree\" structure into the output file\n",
        "def get_features_and_thresholds(tree, features, thresholds):\n",
        "    children = list(tree.values())[0]\n",
        "    if isinstance(children[0], dict):\n",
        "        get_features_and_thresholds(children[0], features, thresholds)\n",
        "    if isinstance(children[1], dict):\n",
        "        get_features_and_thresholds(children[1], features, thresholds)\n",
        "    feature = list(tree.keys())[0].split(' ')[0]\n",
        "    threshold = float(list(tree.keys())[0].split(' ')[2])\n",
        "    features.append(feature)\n",
        "    thresholds.append(threshold)\n",
        "    return\n",
        "\n",
        "ans_features = []\n",
        "ans_thresholds = []\n",
        "get_features_and_thresholds(decisionTree, ans_features, ans_thresholds)\n",
        "ans_features\n",
        "# [Note] You have to save the corresponding thresholds for the features in the \"ans_features\" list into the output file\n",
        "ans_thresholds\n",
        "basic = []\n",
        "basic.append(ans_entropy)\n",
        "basic.append(ans_informationGain)\n",
        "basic.append([ans_ig, ans_value, ans_name])\n",
        "basic.append(ans_left)\n",
        "basic.append(ans_features + ans_thresholds)\n",
        "num_train = 30\n",
        "num_validation = 10\n",
        "\n",
        "training_data = input_data.iloc[:num_train]\n",
        "validation_data = input_data.iloc[-num_validation:]\n",
        "\n",
        "y_train = training_data[['hospital_death']]\n",
        "x_train = training_data.drop(['hospital_death'], axis=1)\n",
        "\n",
        "y_validation = validation_data[['hospital_death']]\n",
        "x_validation = validation_data.drop(['hospital_death'], axis=1)\n",
        "y_validation = y_validation.values.flatten()\n",
        "\n",
        "print(input_data.shape)\n",
        "print(training_data.shape)\n",
        "print(validation_data.shape)\n",
        "max_depth = 2\n",
        "depth = 0\n",
        "min_samples_split = 2\n",
        "n_features = x_train.shape[1]\n",
        "def classify_data(instance, tree):\n",
        "  \"\"\"\n",
        "  This function will predict/classify the input instance\n",
        "  args:\n",
        "  * instance: a instance(case) to be predicted\n",
        "  return:\n",
        "  * answer: the prediction result (the classification result)\n",
        "  \"\"\"\n",
        "  equation = list(tree.keys())[0]\n",
        "  if equation.split()[1] == '<=':\n",
        "    temp_feature = equation.split()[0]\n",
        "    temp_threshold = equation.split()[2]\n",
        "    if instance[temp_feature] > float(temp_threshold):\n",
        "      answer = tree[equation][1]\n",
        "    else:\n",
        "      answer = tree[equation][0]\n",
        "  else:\n",
        "    if instance[equation.split()[0]] in (equation.split()[2]):\n",
        "      answer = tree[equation][0]\n",
        "    else:\n",
        "      answer = tree[equation][1]\n",
        "\n",
        "  if not isinstance(answer, dict):\n",
        "    return answer\n",
        "  else:\n",
        "    return classify_data(instance, answer)\n",
        "\n",
        "\n",
        "def make_prediction(tree, data):\n",
        "  \"\"\"\n",
        "  This function will use your pre-trained decision tree to predict the labels of all instances in data\n",
        "  args:\n",
        "  * tree: the decision tree\n",
        "  * data: the data to predict\n",
        "  return:\n",
        "  * y_prediction: the predictions\n",
        "  \"\"\"\n",
        "  ### START CODE HERE ###\n",
        "  # [Note] You can call the function classify_data() to predict the label of each instance\n",
        "  y_prediction = []\n",
        "  for i in range(data.shape[0]):\n",
        "    y_prediction.append(classify_data(data.iloc[i], tree))\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  return y_prediction\n",
        "\n",
        "\n",
        "def calculate_score(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  This function will calculate the f1-score of the predictions\n",
        "  args:\n",
        "  * y_true: the ground truth\n",
        "  * y_pred: the predictions\n",
        "  return:\n",
        "  * score: the f1-score\n",
        "  \"\"\"\n",
        "  score = f1_score(y_true, y_pred)\n",
        "\n",
        "  return score\n",
        "decision_tree = build_tree(training_data, max_depth, min_samples_split, depth)\n",
        "\n",
        "y_pred = make_prediction(decision_tree, x_validation)\n",
        "\n",
        "# [Note] You have to save the value of \"ans_f1score\" into your output file\n",
        "# Please round your answer to 4 decimal place\n",
        "ans_f1score = calculate_score(y_validation, y_pred)\n",
        "ans_f1score = round(ans_f1score, 4)\n",
        "print(\"ans_f1score = \", ans_f1score)\n",
        "# This is just for you to check your predictions\n",
        "y_pred\n",
        "basic.append(ans_f1score)\n",
        "basic_path = 'lab2_basic.csv'\n",
        "\n",
        "basic_df = pd.DataFrame({'Id': range(len(basic)), 'Ans': basic})\n",
        "basic_df.set_index('Id', inplace=True)\n",
        "basic_df\n",
        "basic_df.to_csv(basic_path, header = True, index = True)\n",
        "advanced_training_data = pd.read_csv('lab2_advanced_training.csv')\n",
        "advanced_training_data\n",
        "advanced_testing_data = pd.read_csv('lab2_advanced_testing.csv')\n",
        "advanced_testing_data\n",
        "### START CODE HERE ###\n",
        "training_data = advanced_training_data\n",
        "validation_data = ...\n",
        "### END CODE HERE ###\n",
        "### START CODE HERE ###\n",
        "# Define the attributes\n",
        "max_depth = 10\n",
        "depth = 0\n",
        "min_samples_split = 5\n",
        "\n",
        "# total number of trees in a random forest\n",
        "n_trees = 500\n",
        "\n",
        "# number of features to train a decision tree\n",
        "n_features = 25\n",
        "\n",
        "# the ratio to select the number of instances\n",
        "sample_size = 0.7\n",
        "n_samples = int(training_data.shape[0] * sample_size)\n",
        "### END CODE HERE ###\n",
        "def build_forest(data, n_trees, n_features, n_samples):\n",
        "  \"\"\"\n",
        "  This function will build a random forest.\n",
        "  args:\n",
        "  * data: all data that can be used to train a random forest\n",
        "  * n_trees: total number of tree\n",
        "  * n_features: number of features\n",
        "  * n_samples: number of instances\n",
        "  return:\n",
        "  * forest: a random forest with 'n_trees' of decision tree\n",
        "  \"\"\"\n",
        "  ### START CODE HERE ###\n",
        "  data_len = data.shape[0]\n",
        "  feature_list = data.columns.tolist()[:-1]\n",
        "  forest = []\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  # Create 'n_trees' number of trees and store each into the 'forest' list\n",
        "  for i in range(n_trees):\n",
        "    \n",
        "    print(f\"Building tree {i+1} out of {n_trees}\")\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # Select 'n_samples' number of samples and 'n_features' number of features\n",
        "    # (you can select randomly or use any other techniques)\n",
        "\n",
        "    selected_datas = random.sample(range(data_len), n_samples)\n",
        "    selected_features = random.sample(feature_list, n_features)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # print(f\"selected_datas = {selected_datas}\")\n",
        "    # print(f\"selected_features = {selected_features}\")\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # Store the rows in 'selected_datas' from 'data' into a new DataFrame\n",
        "    tree_data = pd.DataFrame()\n",
        "    tree_data = data.loc[selected_datas]\n",
        "\n",
        "    # Filter the DataFrame for specific 'selected_features' (columns)\n",
        "    tree_data = data[selected_features + ['hospital_death']]\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Then use the new data and 'build_tree' function to build a tree\n",
        "    tree = build_tree(tree_data, max_depth, min_samples_split, depth)\n",
        "    # print(tree)\n",
        "    print(f\"Tree {i+1} is built\")\n",
        "\n",
        "    # Save your tree\n",
        "    forest.append(tree)\n",
        "\n",
        "  return forest\n",
        "forest = build_forest(training_data, n_trees, n_features, n_samples)\n",
        "def make_prediction_forest(forest, data):\n",
        "  \"\"\"\n",
        "  This function will use the pre-trained random forest to make the predictions\n",
        "  args:\n",
        "  * forest: the random forest\n",
        "  * data: the data used to predict\n",
        "  return:\n",
        "  * y_prediction: the predicted results\n",
        "  \"\"\"\n",
        "  y_prediction = []\n",
        "  predictions = []\n",
        "\n",
        "  ### START CODE HERE ###\n",
        "  # Loop through each tree in the forest\n",
        "  for tree in forest:\n",
        "    # Call 'make_prediction'\n",
        "    pred = make_prediction(tree, data)\n",
        "    predictions.append(pred)\n",
        "\n",
        "  # Here, each tree has made its predictions.\n",
        "  # We can use majority vote in which the final prediction is determined by the mode (most frequent prediction) across all the trees.\n",
        "  # Feel free to use any other method to determine the final prediction\n",
        "\n",
        "  # Loop through each column of 'predictions'\n",
        "  for col in range(len(predictions[0])):\n",
        "    # For a specific column, find out each tree's prediction\n",
        "    column_predictions = np.array(predictions)[:, col]\n",
        "    # Then, use a method to determine the final prediction for this column\n",
        "    # append the final prediction to y_prediction\n",
        "    if sum(column_predictions) / len(column_predictions) >= 0.5:\n",
        "      y_prediction.append(1)\n",
        "    else:\n",
        "      y_prediction.append(0)\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "\n",
        "\n",
        "  return y_prediction\n",
        "### START CODE HERE ###\n",
        "# pred_validation = make_prediction_forest(forest, x_validation)\n",
        "# score = calculate_score(y_validation, pred_validation)\n",
        "# print(score)\n",
        "### END CODE HERE ###\n",
        "y_pred_test = make_prediction_forest(forest, advanced_testing_data)\n",
        "advanced = []\n",
        "for i in range(len(y_pred_test)):\n",
        "  advanced.append(y_pred_test[i])\n",
        "advanced_path = 'lab2_advanced.csv'\n",
        "\n",
        "advanced_df = pd.DataFrame({'Id': range(len(advanced)), 'hospital_death': advanced})\n",
        "advanced_df.set_index('Id', inplace=True)\n",
        "advanced_df\n",
        "advanced_df.to_csv(advanced_path, header = True, index = True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
